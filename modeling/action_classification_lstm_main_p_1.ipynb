{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "action_classification_lstm_main_p_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRu72jqqkev9",
        "outputId": "a591ef79-2f47-4755-f568-e6db4777ad00"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BLeKduqiC6x"
      },
      "source": [
        "# 1. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iJyagkCiC6x"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1DPDkTjS698"
      },
      "source": [
        "## 1) Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEfD5zh-duIy"
      },
      "source": [
        "### (1) Punching\n",
        "\n",
        "- y : 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaj-jf06LWeo"
      },
      "source": [
        "punching_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/punching_sliding.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "70vPpvadcgKT",
        "outputId": "5320ae5f-ec8c-4127-ef0a-ae8d7ae41fe3"
      },
      "source": [
        "df_punching = pd.read_csv(punching_path)\n",
        "\n",
        "df_punching.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>290.693</td>\n",
              "      <td>161.699</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>290.661</td>\n",
              "      <td>158.996</td>\n",
              "      <td>298.499</td>\n",
              "      <td>160.267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.255</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290.692</td>\n",
              "      <td>161.703</td>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>290.662</td>\n",
              "      <td>159.004</td>\n",
              "      <td>298.495</td>\n",
              "      <td>160.273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>291.954</td>\n",
              "      <td>161.696</td>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>290.669</td>\n",
              "      <td>158.995</td>\n",
              "      <td>298.510</td>\n",
              "      <td>160.266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.990</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.694</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>290.658</td>\n",
              "      <td>158.992</td>\n",
              "      <td>298.504</td>\n",
              "      <td>160.261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.802</td>\n",
              "      <td>164.252</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.683</td>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>290.667</td>\n",
              "      <td>158.981</td>\n",
              "      <td>298.509</td>\n",
              "      <td>160.255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.985</td>\n",
              "      <td>164.270</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3        4  ...   32   33       34       35  y\n",
              "0  290.693  161.699  307.602  207.250  285.384  ...  0.0  0.0  317.984  164.255  2\n",
              "1  290.692  161.703  307.593  205.975  284.154  ...  0.0  0.0  317.984  164.251  2\n",
              "2  291.954  161.696  307.587  207.241  284.136  ...  0.0  0.0  317.990  164.251  2\n",
              "3  290.694  161.694  307.602  207.250  284.168  ...  0.0  0.0  316.802  164.252  2\n",
              "4  290.694  161.683  307.600  207.254  284.173  ...  0.0  0.0  317.985  164.270  2\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glILS8CxdyFv"
      },
      "source": [
        "### (2) Smoking\n",
        "\n",
        "- y : 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs0clNtcfJpV"
      },
      "source": [
        "smoking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/smoking_sliding.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "clOyg9k8L31I",
        "outputId": "06c50550-5a96-4de5-8927-ad8d0eec6ff5"
      },
      "source": [
        "df_smoking = pd.read_csv(smoking_path)\n",
        "\n",
        "df_smoking.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>333.770000</td>\n",
              "      <td>85.241778</td>\n",
              "      <td>324.893667</td>\n",
              "      <td>117.939111</td>\n",
              "      <td>305.293000</td>\n",
              "      <td>119.228000</td>\n",
              "      <td>295.470667</td>\n",
              "      <td>162.394222</td>\n",
              "      <td>297.418667</td>\n",
              "      <td>201.668889</td>\n",
              "      <td>341.613333</td>\n",
              "      <td>116.635111</td>\n",
              "      <td>351.406667</td>\n",
              "      <td>154.591556</td>\n",
              "      <td>349.486667</td>\n",
              "      <td>132.359556</td>\n",
              "      <td>309.233000</td>\n",
              "      <td>204.285778</td>\n",
              "      <td>303.348000</td>\n",
              "      <td>267.131556</td>\n",
              "      <td>302.326000</td>\n",
              "      <td>323.346222</td>\n",
              "      <td>333.786667</td>\n",
              "      <td>204.287556</td>\n",
              "      <td>332.769000</td>\n",
              "      <td>267.078667</td>\n",
              "      <td>326.903333</td>\n",
              "      <td>320.686222</td>\n",
              "      <td>330.772000</td>\n",
              "      <td>78.676444</td>\n",
              "      <td>337.650000</td>\n",
              "      <td>78.681778</td>\n",
              "      <td>951.296</td>\n",
              "      <td>180.159</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>333.790000</td>\n",
              "      <td>85.257778</td>\n",
              "      <td>324.900667</td>\n",
              "      <td>117.922667</td>\n",
              "      <td>306.279667</td>\n",
              "      <td>119.208000</td>\n",
              "      <td>295.499333</td>\n",
              "      <td>162.396889</td>\n",
              "      <td>297.438667</td>\n",
              "      <td>202.923111</td>\n",
              "      <td>341.596667</td>\n",
              "      <td>116.629778</td>\n",
              "      <td>352.383333</td>\n",
              "      <td>153.326222</td>\n",
              "      <td>349.453333</td>\n",
              "      <td>129.706667</td>\n",
              "      <td>309.247667</td>\n",
              "      <td>204.291111</td>\n",
              "      <td>304.301000</td>\n",
              "      <td>267.088000</td>\n",
              "      <td>302.311000</td>\n",
              "      <td>323.341333</td>\n",
              "      <td>333.783333</td>\n",
              "      <td>204.279111</td>\n",
              "      <td>332.776000</td>\n",
              "      <td>267.048444</td>\n",
              "      <td>327.850000</td>\n",
              "      <td>319.456000</td>\n",
              "      <td>330.793667</td>\n",
              "      <td>78.677333</td>\n",
              "      <td>338.630000</td>\n",
              "      <td>78.707111</td>\n",
              "      <td>954.022</td>\n",
              "      <td>180.145</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>334.726667</td>\n",
              "      <td>86.532000</td>\n",
              "      <td>324.929333</td>\n",
              "      <td>117.944444</td>\n",
              "      <td>307.238000</td>\n",
              "      <td>119.218222</td>\n",
              "      <td>295.492333</td>\n",
              "      <td>162.445333</td>\n",
              "      <td>297.437000</td>\n",
              "      <td>203.006222</td>\n",
              "      <td>342.553333</td>\n",
              "      <td>116.648000</td>\n",
              "      <td>356.276667</td>\n",
              "      <td>153.281333</td>\n",
              "      <td>349.423333</td>\n",
              "      <td>127.101333</td>\n",
              "      <td>309.245000</td>\n",
              "      <td>204.280444</td>\n",
              "      <td>304.305333</td>\n",
              "      <td>267.110667</td>\n",
              "      <td>302.328333</td>\n",
              "      <td>323.337333</td>\n",
              "      <td>333.783333</td>\n",
              "      <td>204.265333</td>\n",
              "      <td>332.779667</td>\n",
              "      <td>267.073778</td>\n",
              "      <td>327.848000</td>\n",
              "      <td>319.466667</td>\n",
              "      <td>331.762333</td>\n",
              "      <td>78.708889</td>\n",
              "      <td>339.593333</td>\n",
              "      <td>78.735111</td>\n",
              "      <td>954.154</td>\n",
              "      <td>180.149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>334.750000</td>\n",
              "      <td>86.560000</td>\n",
              "      <td>325.887000</td>\n",
              "      <td>116.626667</td>\n",
              "      <td>308.246000</td>\n",
              "      <td>117.935111</td>\n",
              "      <td>296.467333</td>\n",
              "      <td>161.117333</td>\n",
              "      <td>296.472000</td>\n",
              "      <td>201.634222</td>\n",
              "      <td>344.516667</td>\n",
              "      <td>115.312889</td>\n",
              "      <td>356.326667</td>\n",
              "      <td>153.299111</td>\n",
              "      <td>349.396667</td>\n",
              "      <td>125.814222</td>\n",
              "      <td>309.236667</td>\n",
              "      <td>203.004889</td>\n",
              "      <td>304.324333</td>\n",
              "      <td>267.065333</td>\n",
              "      <td>302.322000</td>\n",
              "      <td>322.046222</td>\n",
              "      <td>334.706667</td>\n",
              "      <td>203.014222</td>\n",
              "      <td>332.779000</td>\n",
              "      <td>267.043111</td>\n",
              "      <td>327.851000</td>\n",
              "      <td>320.692000</td>\n",
              "      <td>331.787000</td>\n",
              "      <td>78.740889</td>\n",
              "      <td>339.616667</td>\n",
              "      <td>78.747556</td>\n",
              "      <td>954.270</td>\n",
              "      <td>183.041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>335.693333</td>\n",
              "      <td>86.576000</td>\n",
              "      <td>325.897333</td>\n",
              "      <td>116.635556</td>\n",
              "      <td>309.174667</td>\n",
              "      <td>117.969778</td>\n",
              "      <td>296.454333</td>\n",
              "      <td>161.153778</td>\n",
              "      <td>296.440000</td>\n",
              "      <td>201.635556</td>\n",
              "      <td>344.510000</td>\n",
              "      <td>114.017778</td>\n",
              "      <td>357.293333</td>\n",
              "      <td>153.281778</td>\n",
              "      <td>348.490000</td>\n",
              "      <td>121.908000</td>\n",
              "      <td>309.245667</td>\n",
              "      <td>202.997778</td>\n",
              "      <td>306.263000</td>\n",
              "      <td>265.787556</td>\n",
              "      <td>302.312667</td>\n",
              "      <td>322.045333</td>\n",
              "      <td>334.703333</td>\n",
              "      <td>203.001333</td>\n",
              "      <td>332.793667</td>\n",
              "      <td>265.820000</td>\n",
              "      <td>326.898667</td>\n",
              "      <td>320.699556</td>\n",
              "      <td>332.704000</td>\n",
              "      <td>78.756000</td>\n",
              "      <td>340.543333</td>\n",
              "      <td>78.752889</td>\n",
              "      <td>957.030</td>\n",
              "      <td>183.052</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0          1           2           3  ...       33   34   35  y\n",
              "0  333.770000  85.241778  324.893667  117.939111  ...  180.159  0.0  0.0  1\n",
              "1  333.790000  85.257778  324.900667  117.922667  ...  180.145  0.0  0.0  1\n",
              "2  334.726667  86.532000  324.929333  117.944444  ...  180.149  0.0  0.0  1\n",
              "3  334.750000  86.560000  325.887000  116.626667  ...  183.041  0.0  0.0  1\n",
              "4  335.693333  86.576000  325.897333  116.635556  ...  183.052  0.0  0.0  1\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K61ouwsPfDtJ"
      },
      "source": [
        "### (3) Walking\n",
        "\n",
        "- y : 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMq2pTeffG9x"
      },
      "source": [
        "walking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/walking_sliding.csv'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "hiUMb5omfSvs",
        "outputId": "db4620f9-b3c5-48c2-96d0-acc7cce85cb6"
      },
      "source": [
        "df_walking = pd.read_csv(walking_path)\n",
        "\n",
        "df_walking.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.591563</td>\n",
              "      <td>268.375111</td>\n",
              "      <td>34.471000</td>\n",
              "      <td>318.115556</td>\n",
              "      <td>18.768733</td>\n",
              "      <td>412.252000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.119073</td>\n",
              "      <td>276.197333</td>\n",
              "      <td>38.427667</td>\n",
              "      <td>324.640444</td>\n",
              "      <td>40.388333</td>\n",
              "      <td>413.647111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.973267</td>\n",
              "      <td>261.836000</td>\n",
              "      <td>35.534667</td>\n",
              "      <td>331.182667</td>\n",
              "      <td>56.094333</td>\n",
              "      <td>409.643111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.865767</td>\n",
              "      <td>48.626222</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.950300</td>\n",
              "      <td>36.842400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.81100</td>\n",
              "      <td>74.0119</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25.678567</td>\n",
              "      <td>46.038667</td>\n",
              "      <td>2.602713</td>\n",
              "      <td>87.830667</td>\n",
              "      <td>2.610417</td>\n",
              "      <td>87.808889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.9581</td>\n",
              "      <td>87.867111</td>\n",
              "      <td>18.7978</td>\n",
              "      <td>214.746667</td>\n",
              "      <td>57.039</td>\n",
              "      <td>244.859556</td>\n",
              "      <td>9.970833</td>\n",
              "      <td>251.376889</td>\n",
              "      <td>51.176333</td>\n",
              "      <td>325.939111</td>\n",
              "      <td>90.450667</td>\n",
              "      <td>410.959111</td>\n",
              "      <td>12.930667</td>\n",
              "      <td>246.168444</td>\n",
              "      <td>53.159000</td>\n",
              "      <td>325.941778</td>\n",
              "      <td>91.442333</td>\n",
              "      <td>407.053333</td>\n",
              "      <td>19.800967</td>\n",
              "      <td>36.828222</td>\n",
              "      <td>26.649067</td>\n",
              "      <td>38.136844</td>\n",
              "      <td>7.78672</td>\n",
              "      <td>82.8243</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0          1         2          3  ...       33   34   35  y\n",
              "0   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "1   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "2   0.000000   0.000000  0.000000   0.000000  ...   0.0000  0.0  0.0  0\n",
              "3  14.865767  48.626222  0.000000   0.000000  ...  74.0119  0.0  0.0  0\n",
              "4  25.678567  46.038667  2.602713  87.830667  ...  82.8243  0.0  0.0  0\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUS48VoXEWoN"
      },
      "source": [
        "### (4) Running\n",
        "\n",
        "- y : 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzJRAHywEcJK"
      },
      "source": [
        "running_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/running_sliding.csv'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "csOwXUtyEf6s",
        "outputId": "91be409c-6fc7-431d-d07b-44dc5d1fbf8b"
      },
      "source": [
        "df_running = pd.read_csv(running_path)\n",
        "df_running['y'] = 3\n",
        "df_running.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>325.900333</td>\n",
              "      <td>124.469778</td>\n",
              "      <td>318.036000</td>\n",
              "      <td>146.749333</td>\n",
              "      <td>308.189333</td>\n",
              "      <td>146.718222</td>\n",
              "      <td>310.156000</td>\n",
              "      <td>170.284444</td>\n",
              "      <td>325.919333</td>\n",
              "      <td>161.073333</td>\n",
              "      <td>328.830000</td>\n",
              "      <td>148.038667</td>\n",
              "      <td>340.590000</td>\n",
              "      <td>172.875111</td>\n",
              "      <td>334.736667</td>\n",
              "      <td>161.076889</td>\n",
              "      <td>311.188667</td>\n",
              "      <td>200.336444</td>\n",
              "      <td>310.189333</td>\n",
              "      <td>240.907556</td>\n",
              "      <td>309.188000</td>\n",
              "      <td>280.174667</td>\n",
              "      <td>327.853000</td>\n",
              "      <td>201.628889</td>\n",
              "      <td>325.877000</td>\n",
              "      <td>243.486667</td>\n",
              "      <td>322.958667</td>\n",
              "      <td>280.183111</td>\n",
              "      <td>323.943667</td>\n",
              "      <td>120.527556</td>\n",
              "      <td>326.896000</td>\n",
              "      <td>120.584444</td>\n",
              "      <td>948.193</td>\n",
              "      <td>274.182</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>325.903000</td>\n",
              "      <td>123.200000</td>\n",
              "      <td>318.053333</td>\n",
              "      <td>146.752000</td>\n",
              "      <td>308.189667</td>\n",
              "      <td>146.707111</td>\n",
              "      <td>310.153333</td>\n",
              "      <td>170.289333</td>\n",
              "      <td>325.910000</td>\n",
              "      <td>161.082667</td>\n",
              "      <td>330.769000</td>\n",
              "      <td>148.043556</td>\n",
              "      <td>340.646667</td>\n",
              "      <td>172.876444</td>\n",
              "      <td>335.683333</td>\n",
              "      <td>161.121333</td>\n",
              "      <td>311.207000</td>\n",
              "      <td>200.380889</td>\n",
              "      <td>310.202000</td>\n",
              "      <td>240.925333</td>\n",
              "      <td>309.192000</td>\n",
              "      <td>280.164889</td>\n",
              "      <td>328.833333</td>\n",
              "      <td>201.680889</td>\n",
              "      <td>325.884000</td>\n",
              "      <td>243.501778</td>\n",
              "      <td>323.884000</td>\n",
              "      <td>280.173333</td>\n",
              "      <td>323.938333</td>\n",
              "      <td>120.514222</td>\n",
              "      <td>326.901000</td>\n",
              "      <td>120.569333</td>\n",
              "      <td>948.198</td>\n",
              "      <td>271.443</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>325.922667</td>\n",
              "      <td>123.186667</td>\n",
              "      <td>318.065333</td>\n",
              "      <td>148.030667</td>\n",
              "      <td>307.251667</td>\n",
              "      <td>146.726222</td>\n",
              "      <td>310.170333</td>\n",
              "      <td>171.532889</td>\n",
              "      <td>326.864667</td>\n",
              "      <td>161.104889</td>\n",
              "      <td>331.738333</td>\n",
              "      <td>149.313333</td>\n",
              "      <td>340.646667</td>\n",
              "      <td>172.936444</td>\n",
              "      <td>337.663333</td>\n",
              "      <td>162.415556</td>\n",
              "      <td>311.202000</td>\n",
              "      <td>201.646222</td>\n",
              "      <td>310.202667</td>\n",
              "      <td>242.205778</td>\n",
              "      <td>309.188667</td>\n",
              "      <td>280.180444</td>\n",
              "      <td>328.832667</td>\n",
              "      <td>202.920444</td>\n",
              "      <td>325.897667</td>\n",
              "      <td>243.519111</td>\n",
              "      <td>323.887000</td>\n",
              "      <td>280.178667</td>\n",
              "      <td>324.858667</td>\n",
              "      <td>119.318222</td>\n",
              "      <td>327.836667</td>\n",
              "      <td>120.565333</td>\n",
              "      <td>948.231</td>\n",
              "      <td>271.429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>325.927000</td>\n",
              "      <td>123.180000</td>\n",
              "      <td>318.074000</td>\n",
              "      <td>148.011111</td>\n",
              "      <td>308.189333</td>\n",
              "      <td>146.708889</td>\n",
              "      <td>310.176667</td>\n",
              "      <td>171.586222</td>\n",
              "      <td>326.862333</td>\n",
              "      <td>161.103111</td>\n",
              "      <td>331.748667</td>\n",
              "      <td>149.312889</td>\n",
              "      <td>340.630000</td>\n",
              "      <td>174.172889</td>\n",
              "      <td>337.660000</td>\n",
              "      <td>159.832889</td>\n",
              "      <td>312.145000</td>\n",
              "      <td>201.636889</td>\n",
              "      <td>310.212333</td>\n",
              "      <td>242.233333</td>\n",
              "      <td>309.204667</td>\n",
              "      <td>280.169778</td>\n",
              "      <td>329.793333</td>\n",
              "      <td>202.911556</td>\n",
              "      <td>325.906000</td>\n",
              "      <td>243.528000</td>\n",
              "      <td>322.959333</td>\n",
              "      <td>280.157333</td>\n",
              "      <td>324.864333</td>\n",
              "      <td>119.316444</td>\n",
              "      <td>327.837333</td>\n",
              "      <td>120.555556</td>\n",
              "      <td>948.276</td>\n",
              "      <td>271.415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>326.850333</td>\n",
              "      <td>123.169778</td>\n",
              "      <td>319.001667</td>\n",
              "      <td>148.024444</td>\n",
              "      <td>307.250000</td>\n",
              "      <td>146.709778</td>\n",
              "      <td>309.241667</td>\n",
              "      <td>171.608000</td>\n",
              "      <td>325.923333</td>\n",
              "      <td>161.119111</td>\n",
              "      <td>332.717667</td>\n",
              "      <td>149.334667</td>\n",
              "      <td>340.606667</td>\n",
              "      <td>174.235556</td>\n",
              "      <td>338.636667</td>\n",
              "      <td>162.440444</td>\n",
              "      <td>311.200000</td>\n",
              "      <td>201.633333</td>\n",
              "      <td>310.226000</td>\n",
              "      <td>242.231556</td>\n",
              "      <td>309.211667</td>\n",
              "      <td>280.166222</td>\n",
              "      <td>329.794333</td>\n",
              "      <td>201.690667</td>\n",
              "      <td>325.908667</td>\n",
              "      <td>243.503556</td>\n",
              "      <td>322.957667</td>\n",
              "      <td>280.145778</td>\n",
              "      <td>324.881000</td>\n",
              "      <td>119.313778</td>\n",
              "      <td>327.863000</td>\n",
              "      <td>120.553778</td>\n",
              "      <td>951.038</td>\n",
              "      <td>271.414</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2           3  ...       33   34   35  y\n",
              "0  325.900333  124.469778  318.036000  146.749333  ...  274.182  0.0  0.0  3\n",
              "1  325.903000  123.200000  318.053333  146.752000  ...  271.443  0.0  0.0  3\n",
              "2  325.922667  123.186667  318.065333  148.030667  ...  271.429  0.0  0.0  3\n",
              "3  325.927000  123.180000  318.074000  148.011111  ...  271.415  0.0  0.0  3\n",
              "4  326.850333  123.169778  319.001667  148.024444  ...  271.414  0.0  0.0  3\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLkryM51Qt2t"
      },
      "source": [
        "### (5) Kicking\n",
        "- y : 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiU1uE4QRdYi"
      },
      "source": [
        "kicking_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/kicking_sliding.csv'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Jgyd2hemRdgd",
        "outputId": "85a4bfc2-b90d-4d40-ef77-5ef1fca36842"
      },
      "source": [
        "df_kicking = pd.read_csv(kicking_path)\n",
        "df_kicking['y'] = 4\n",
        "df_kicking.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>226.751000</td>\n",
              "      <td>8.047956</td>\n",
              "      <td>231.340333</td>\n",
              "      <td>50.718222</td>\n",
              "      <td>206.518333</td>\n",
              "      <td>45.540000</td>\n",
              "      <td>179.081333</td>\n",
              "      <td>77.709333</td>\n",
              "      <td>175.216333</td>\n",
              "      <td>65.553778</td>\n",
              "      <td>255.503000</td>\n",
              "      <td>52.476889</td>\n",
              "      <td>268.586333</td>\n",
              "      <td>99.458222</td>\n",
              "      <td>273.788000</td>\n",
              "      <td>142.175556</td>\n",
              "      <td>212.417333</td>\n",
              "      <td>149.987556</td>\n",
              "      <td>221.521000</td>\n",
              "      <td>216.165333</td>\n",
              "      <td>227.423000</td>\n",
              "      <td>266.641778</td>\n",
              "      <td>237.855333</td>\n",
              "      <td>153.462222</td>\n",
              "      <td>236.557000</td>\n",
              "      <td>223.126222</td>\n",
              "      <td>237.865333</td>\n",
              "      <td>275.322222</td>\n",
              "      <td>222.223000</td>\n",
              "      <td>2.377347</td>\n",
              "      <td>232.009333</td>\n",
              "      <td>2.379378</td>\n",
              "      <td>652.956</td>\n",
              "      <td>5.34914</td>\n",
              "      <td>735.208</td>\n",
              "      <td>5.34283</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>228.073667</td>\n",
              "      <td>8.893156</td>\n",
              "      <td>232.000000</td>\n",
              "      <td>50.710667</td>\n",
              "      <td>207.168333</td>\n",
              "      <td>45.508444</td>\n",
              "      <td>179.078000</td>\n",
              "      <td>75.108889</td>\n",
              "      <td>181.073000</td>\n",
              "      <td>58.553333</td>\n",
              "      <td>257.446000</td>\n",
              "      <td>52.501778</td>\n",
              "      <td>269.202667</td>\n",
              "      <td>99.459111</td>\n",
              "      <td>274.428667</td>\n",
              "      <td>141.272444</td>\n",
              "      <td>211.780333</td>\n",
              "      <td>151.723556</td>\n",
              "      <td>218.936667</td>\n",
              "      <td>217.913778</td>\n",
              "      <td>227.405000</td>\n",
              "      <td>266.656889</td>\n",
              "      <td>237.867333</td>\n",
              "      <td>155.196444</td>\n",
              "      <td>236.545333</td>\n",
              "      <td>224.844889</td>\n",
              "      <td>237.193667</td>\n",
              "      <td>298.020000</td>\n",
              "      <td>225.459000</td>\n",
              "      <td>2.379049</td>\n",
              "      <td>233.967000</td>\n",
              "      <td>2.379662</td>\n",
              "      <td>660.705</td>\n",
              "      <td>5.35190</td>\n",
              "      <td>742.914</td>\n",
              "      <td>5.33893</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>231.341667</td>\n",
              "      <td>8.944178</td>\n",
              "      <td>232.646667</td>\n",
              "      <td>51.578222</td>\n",
              "      <td>208.482333</td>\n",
              "      <td>45.532444</td>\n",
              "      <td>179.086333</td>\n",
              "      <td>73.365333</td>\n",
              "      <td>188.907333</td>\n",
              "      <td>51.592444</td>\n",
              "      <td>258.088667</td>\n",
              "      <td>53.362667</td>\n",
              "      <td>269.207667</td>\n",
              "      <td>98.618222</td>\n",
              "      <td>275.091667</td>\n",
              "      <td>138.657333</td>\n",
              "      <td>211.751000</td>\n",
              "      <td>151.732000</td>\n",
              "      <td>216.981667</td>\n",
              "      <td>217.916889</td>\n",
              "      <td>226.794667</td>\n",
              "      <td>266.660000</td>\n",
              "      <td>237.892667</td>\n",
              "      <td>155.200444</td>\n",
              "      <td>237.195333</td>\n",
              "      <td>223.127556</td>\n",
              "      <td>236.565667</td>\n",
              "      <td>299.737333</td>\n",
              "      <td>226.784667</td>\n",
              "      <td>2.373804</td>\n",
              "      <td>237.206667</td>\n",
              "      <td>2.377391</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>744.950</td>\n",
              "      <td>5.33622</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>232.662667</td>\n",
              "      <td>8.921378</td>\n",
              "      <td>232.676000</td>\n",
              "      <td>51.603556</td>\n",
              "      <td>209.784333</td>\n",
              "      <td>46.368889</td>\n",
              "      <td>180.414333</td>\n",
              "      <td>72.452444</td>\n",
              "      <td>196.113667</td>\n",
              "      <td>46.384444</td>\n",
              "      <td>258.115000</td>\n",
              "      <td>55.066222</td>\n",
              "      <td>269.212000</td>\n",
              "      <td>98.622667</td>\n",
              "      <td>277.047667</td>\n",
              "      <td>135.199556</td>\n",
              "      <td>211.744667</td>\n",
              "      <td>150.847556</td>\n",
              "      <td>213.068333</td>\n",
              "      <td>217.900000</td>\n",
              "      <td>226.759667</td>\n",
              "      <td>264.060444</td>\n",
              "      <td>239.191333</td>\n",
              "      <td>155.192889</td>\n",
              "      <td>237.236000</td>\n",
              "      <td>219.679111</td>\n",
              "      <td>236.570333</td>\n",
              "      <td>300.612444</td>\n",
              "      <td>228.101000</td>\n",
              "      <td>2.375556</td>\n",
              "      <td>238.523000</td>\n",
              "      <td>2.378511</td>\n",
              "      <td>666.620</td>\n",
              "      <td>14.17530</td>\n",
              "      <td>748.852</td>\n",
              "      <td>5.33723</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>233.963667</td>\n",
              "      <td>9.754933</td>\n",
              "      <td>234.606000</td>\n",
              "      <td>51.568889</td>\n",
              "      <td>211.745333</td>\n",
              "      <td>45.517778</td>\n",
              "      <td>183.660667</td>\n",
              "      <td>72.451111</td>\n",
              "      <td>204.578000</td>\n",
              "      <td>46.396889</td>\n",
              "      <td>258.752000</td>\n",
              "      <td>55.080889</td>\n",
              "      <td>269.209333</td>\n",
              "      <td>99.467556</td>\n",
              "      <td>278.362667</td>\n",
              "      <td>131.711111</td>\n",
              "      <td>211.760333</td>\n",
              "      <td>151.734222</td>\n",
              "      <td>207.840333</td>\n",
              "      <td>219.661333</td>\n",
              "      <td>222.855333</td>\n",
              "      <td>256.222667</td>\n",
              "      <td>241.772333</td>\n",
              "      <td>156.074667</td>\n",
              "      <td>239.183000</td>\n",
              "      <td>234.444000</td>\n",
              "      <td>236.565667</td>\n",
              "      <td>301.473778</td>\n",
              "      <td>230.686333</td>\n",
              "      <td>2.374538</td>\n",
              "      <td>240.484667</td>\n",
              "      <td>2.377596</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>752.783</td>\n",
              "      <td>5.33532</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1           2          3  ...        33       34       35  y\n",
              "0  226.751000  8.047956  231.340333  50.718222  ...   5.34914  735.208  5.34283  4\n",
              "1  228.073667  8.893156  232.000000  50.710667  ...   5.35190  742.914  5.33893  4\n",
              "2  231.341667  8.944178  232.646667  51.578222  ...   0.00000  744.950  5.33622  4\n",
              "3  232.662667  8.921378  232.676000  51.603556  ...  14.17530  748.852  5.33723  4\n",
              "4  233.963667  9.754933  234.606000  51.568889  ...   0.00000  752.783  5.33532  4\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWSTZ_gkUpbw"
      },
      "source": [
        "## (6) Standing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncphDgm3V1p_"
      },
      "source": [
        "standing_path ='/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/standing_sliding.csv'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "vQnCfFLJVvvN",
        "outputId": "91a643cb-62dc-4169-a48e-5108a6ca22af"
      },
      "source": [
        "df_standing = pd.read_csv(standing_path)\n",
        "df_standing['y'] = 5\n",
        "df_standing.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>153.192333</td>\n",
              "      <td>288.022667</td>\n",
              "      <td>216.022000</td>\n",
              "      <td>244.815556</td>\n",
              "      <td>208.150667</td>\n",
              "      <td>239.608889</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>223.861333</td>\n",
              "      <td>246.135556</td>\n",
              "      <td>299.396333</td>\n",
              "      <td>263.126667</td>\n",
              "      <td>363.166667</td>\n",
              "      <td>297.138667</td>\n",
              "      <td>336.683333</td>\n",
              "      <td>256.606667</td>\n",
              "      <td>428.893333</td>\n",
              "      <td>248.765778</td>\n",
              "      <td>527.016667</td>\n",
              "      <td>246.147556</td>\n",
              "      <td>353.353333</td>\n",
              "      <td>225.231556</td>\n",
              "      <td>430.860000</td>\n",
              "      <td>246.186222</td>\n",
              "      <td>543.680000</td>\n",
              "      <td>234.351111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>147.341000</td>\n",
              "      <td>277.553778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>471.438</td>\n",
              "      <td>556.854</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>152.204333</td>\n",
              "      <td>288.059556</td>\n",
              "      <td>216.022667</td>\n",
              "      <td>244.818222</td>\n",
              "      <td>208.131667</td>\n",
              "      <td>239.616444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>223.876000</td>\n",
              "      <td>246.140444</td>\n",
              "      <td>299.401000</td>\n",
              "      <td>260.545778</td>\n",
              "      <td>364.110000</td>\n",
              "      <td>295.855111</td>\n",
              "      <td>335.696667</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>428.903333</td>\n",
              "      <td>248.752444</td>\n",
              "      <td>521.133333</td>\n",
              "      <td>252.697333</td>\n",
              "      <td>351.403333</td>\n",
              "      <td>225.216444</td>\n",
              "      <td>430.866667</td>\n",
              "      <td>246.151556</td>\n",
              "      <td>543.673333</td>\n",
              "      <td>233.080000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>146.379000</td>\n",
              "      <td>277.592444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>471.416</td>\n",
              "      <td>556.885</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>152.232333</td>\n",
              "      <td>288.067111</td>\n",
              "      <td>216.000667</td>\n",
              "      <td>244.830222</td>\n",
              "      <td>207.186667</td>\n",
              "      <td>242.205778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>223.843333</td>\n",
              "      <td>246.116000</td>\n",
              "      <td>299.390000</td>\n",
              "      <td>260.544889</td>\n",
              "      <td>364.163333</td>\n",
              "      <td>294.548444</td>\n",
              "      <td>333.736667</td>\n",
              "      <td>264.435556</td>\n",
              "      <td>423.030000</td>\n",
              "      <td>244.846667</td>\n",
              "      <td>538.770000</td>\n",
              "      <td>233.072889</td>\n",
              "      <td>352.370000</td>\n",
              "      <td>226.530667</td>\n",
              "      <td>429.876667</td>\n",
              "      <td>246.138222</td>\n",
              "      <td>544.630000</td>\n",
              "      <td>231.778667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>147.351000</td>\n",
              "      <td>278.830667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>471.440</td>\n",
              "      <td>559.761</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>153.215000</td>\n",
              "      <td>288.028889</td>\n",
              "      <td>215.040333</td>\n",
              "      <td>244.840444</td>\n",
              "      <td>207.153667</td>\n",
              "      <td>243.493778</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>222.888000</td>\n",
              "      <td>246.126667</td>\n",
              "      <td>299.376000</td>\n",
              "      <td>259.214222</td>\n",
              "      <td>364.183333</td>\n",
              "      <td>289.359111</td>\n",
              "      <td>328.831333</td>\n",
              "      <td>260.535556</td>\n",
              "      <td>426.936667</td>\n",
              "      <td>240.919111</td>\n",
              "      <td>536.840000</td>\n",
              "      <td>234.348000</td>\n",
              "      <td>350.433333</td>\n",
              "      <td>225.210222</td>\n",
              "      <td>429.873333</td>\n",
              "      <td>244.865778</td>\n",
              "      <td>539.753333</td>\n",
              "      <td>234.336000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>147.349667</td>\n",
              "      <td>277.603111</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>471.425</td>\n",
              "      <td>559.783</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>151.264000</td>\n",
              "      <td>289.293333</td>\n",
              "      <td>215.031000</td>\n",
              "      <td>244.821778</td>\n",
              "      <td>207.153667</td>\n",
              "      <td>242.202222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>222.880333</td>\n",
              "      <td>246.120444</td>\n",
              "      <td>300.381667</td>\n",
              "      <td>257.953333</td>\n",
              "      <td>365.130000</td>\n",
              "      <td>288.043556</td>\n",
              "      <td>330.790333</td>\n",
              "      <td>264.447111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>350.403333</td>\n",
              "      <td>225.230222</td>\n",
              "      <td>431.840000</td>\n",
              "      <td>246.131111</td>\n",
              "      <td>541.730000</td>\n",
              "      <td>234.374667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>146.348667</td>\n",
              "      <td>278.838222</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>471.427</td>\n",
              "      <td>562.703</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2           3  ...   33       34       35  y\n",
              "0  153.192333  288.022667  216.022000  244.815556  ...  0.0  471.438  556.854  5\n",
              "1  152.204333  288.059556  216.022667  244.818222  ...  0.0  471.416  556.885  5\n",
              "2  152.232333  288.067111  216.000667  244.830222  ...  0.0  471.440  559.761  5\n",
              "3  153.215000  288.028889  215.040333  244.840444  ...  0.0  471.425  559.783  5\n",
              "4  151.264000  289.293333  215.031000  244.821778  ...  0.0  471.427  562.703  5\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxu9Jgb76p9H"
      },
      "source": [
        "## 2) Check Null Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A59c0QCEtDtE",
        "outputId": "ae9148a3-36b1-4187-c541-627124f342cb"
      },
      "source": [
        "df_punching.isnull().sum().sum()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKzkGN87tXKJ",
        "outputId": "403bcec7-b115-49ad-cb13-8c648bac615d"
      },
      "source": [
        "df_smoking.isnull().sum().sum()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK3ovUmvfeWC",
        "outputId": "f7f9d61a-a5e0-4b1a-ceb1-73bffc0f26f4"
      },
      "source": [
        "df_walking.isnull().sum().sum()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JlLj2lWFR0L",
        "outputId": "55bc927e-47d1-46c0-8a79-2ba898195799"
      },
      "source": [
        "df_running.isnull().sum().sum()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz66vx6uRrje",
        "outputId": "9d0d0848-c1f9-4e14-c883-22f6507406fb"
      },
      "source": [
        "df_kicking.isnull().sum().sum()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiFdYTGAW3VT",
        "outputId": "abeccdb8-5af2-4539-a3e6-47ecd08dbc05"
      },
      "source": [
        "df_standing.isnull().sum().sum()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qE8J35cOIFM"
      },
      "source": [
        "## 3) Concatenate Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxdeYglpOk-2",
        "outputId": "824e1fdc-5bcd-41ce-da88-dc6662ae1581"
      },
      "source": [
        "print(df_punching.shape)\n",
        "print(df_smoking.shape)\n",
        "print(df_walking.shape)\n",
        "print(df_running.shape)\n",
        "print(df_kicking.shape)\n",
        "print(df_standing.shape)\n",
        "print(df_punching.shape[0] + df_smoking.shape[0] + df_walking.shape[0] + df_running.shape[0] + df_kicking.shape[0] + df_standing.shape[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(190080, 37)\n",
            "(2141152, 37)\n",
            "(2234848, 37)\n",
            "(2866784, 37)\n",
            "(160288, 37)\n",
            "(142464, 37)\n",
            "7735616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "RCAhgJw3ONsM",
        "outputId": "1ef5ddb6-5ee3-448a-9866-12d2183e33f4"
      },
      "source": [
        "df = pd.concat((df_punching, df_smoking, df_walking, df_running, df_kicking, df_standing), axis=0, ignore_index=True)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>290.693</td>\n",
              "      <td>161.699</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>290.661</td>\n",
              "      <td>158.996</td>\n",
              "      <td>298.499</td>\n",
              "      <td>160.267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.255</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290.692</td>\n",
              "      <td>161.703</td>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>290.662</td>\n",
              "      <td>159.004</td>\n",
              "      <td>298.495</td>\n",
              "      <td>160.273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.984</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>291.954</td>\n",
              "      <td>161.696</td>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>290.669</td>\n",
              "      <td>158.995</td>\n",
              "      <td>298.510</td>\n",
              "      <td>160.266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.990</td>\n",
              "      <td>164.251</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.694</td>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>290.658</td>\n",
              "      <td>158.992</td>\n",
              "      <td>298.504</td>\n",
              "      <td>160.261</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.802</td>\n",
              "      <td>164.252</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>290.694</td>\n",
              "      <td>161.683</td>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>290.667</td>\n",
              "      <td>158.981</td>\n",
              "      <td>298.509</td>\n",
              "      <td>160.255</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>317.985</td>\n",
              "      <td>164.270</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3        4  ...   32   33       34       35  y\n",
              "0  290.693  161.699  307.602  207.250  285.384  ...  0.0  0.0  317.984  164.255  2\n",
              "1  290.692  161.703  307.593  205.975  284.154  ...  0.0  0.0  317.984  164.251  2\n",
              "2  291.954  161.696  307.587  207.241  284.136  ...  0.0  0.0  317.990  164.251  2\n",
              "3  290.694  161.694  307.602  207.250  284.168  ...  0.0  0.0  316.802  164.252  2\n",
              "4  290.694  161.683  307.600  207.254  284.173  ...  0.0  0.0  317.985  164.270  2\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfaX5mhgO0LG",
        "outputId": "ed23b1e5-e5f0-487e-f0e7-210beb501371"
      },
      "source": [
        "df.shape[0] / 32"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "241738.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guk0W6FHqp_V",
        "outputId": "4a1ea842-00f7-44cc-d44d-c788d37afca5"
      },
      "source": [
        "df['y'].value_counts()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2866784\n",
              "0    2234848\n",
              "1    2141152\n",
              "2     190080\n",
              "4     160288\n",
              "5     142464\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnGB3EbWt_uI"
      },
      "source": [
        "## 4) Joint(x, y) Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alY3YQCCvjFH"
      },
      "source": [
        "#### (1) Delete Nose, Eye, Ear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxuuinSdu01a"
      },
      "source": [
        "```\n",
        "{0,  \"Nose\"}, -> '0', '1' delete\n",
        "{1,  \"Neck\"}, -> '2', '3'\n",
        "{2,  \"RShoulder\"}, -> '4', '5'\n",
        "{3,  \"RElbow\"}, -> '6', '7'\n",
        "{4,  \"RWrist\"}, -> '8', '9'\n",
        "{5,  \"LShoulder\"}, -> '10', '11'\n",
        "{6,  \"LElbow\"}, -> '12', '13'\n",
        "{7,  \"LWrist\"}, -> '14', '15'\n",
        "{8,  \"RHip\"}, -> '16', '17'\n",
        "{9,  \"RKnee\"}, -> '18', '19'\n",
        "{10, \"RAnkle\"}, -> '20', '21'\n",
        "{11, \"LHip\"}, -> '22', '23'\n",
        "{12, \"LKnee\"}, -> '24', '25'\n",
        "{13, \"LAnkle\"}, -> '26', '27'\n",
        "{14, \"REye\"}, -> '28', '29' delete\n",
        "{15, \"LEye\"}, -> '30', '31' delete\n",
        "{16, \"REar\"}, -> '32', '33' delete\n",
        "{17, \"LEar\"}, -> '34', '35' delete\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHQWcrQuDwO"
      },
      "source": [
        "def deleteJoint(df):\n",
        "    df.drop(['0', '1'] + [str(i) for i in range(28, 36)], axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VtzslML_uSCM",
        "outputId": "362e4557-4779-4792-ff9a-5a722777b47f"
      },
      "source": [
        "df = deleteJoint(df)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>285.384</td>\n",
              "      <td>205.939</td>\n",
              "      <td>275.033</td>\n",
              "      <td>250.267</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.746</td>\n",
              "      <td>331.090</td>\n",
              "      <td>211.121</td>\n",
              "      <td>337.569</td>\n",
              "      <td>259.385</td>\n",
              "      <td>327.208</td>\n",
              "      <td>295.910</td>\n",
              "      <td>286.718</td>\n",
              "      <td>291.991</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.547</td>\n",
              "      <td>289.371</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.035</td>\n",
              "      <td>294.563</td>\n",
              "      <td>328.432</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.620</td>\n",
              "      <td>419.732</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>307.593</td>\n",
              "      <td>205.975</td>\n",
              "      <td>284.154</td>\n",
              "      <td>204.656</td>\n",
              "      <td>275.023</td>\n",
              "      <td>250.258</td>\n",
              "      <td>265.927</td>\n",
              "      <td>286.758</td>\n",
              "      <td>331.096</td>\n",
              "      <td>211.115</td>\n",
              "      <td>337.552</td>\n",
              "      <td>263.272</td>\n",
              "      <td>325.892</td>\n",
              "      <td>298.512</td>\n",
              "      <td>285.515</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.964</td>\n",
              "      <td>354.542</td>\n",
              "      <td>289.346</td>\n",
              "      <td>401.530</td>\n",
              "      <td>318.033</td>\n",
              "      <td>294.569</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.463</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.964</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>307.587</td>\n",
              "      <td>207.241</td>\n",
              "      <td>284.136</td>\n",
              "      <td>204.675</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.537</td>\n",
              "      <td>265.938</td>\n",
              "      <td>286.740</td>\n",
              "      <td>331.109</td>\n",
              "      <td>211.125</td>\n",
              "      <td>337.554</td>\n",
              "      <td>263.290</td>\n",
              "      <td>325.854</td>\n",
              "      <td>298.535</td>\n",
              "      <td>285.498</td>\n",
              "      <td>291.992</td>\n",
              "      <td>291.975</td>\n",
              "      <td>354.535</td>\n",
              "      <td>289.367</td>\n",
              "      <td>401.528</td>\n",
              "      <td>318.019</td>\n",
              "      <td>294.574</td>\n",
              "      <td>328.420</td>\n",
              "      <td>358.453</td>\n",
              "      <td>337.624</td>\n",
              "      <td>420.957</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>307.602</td>\n",
              "      <td>207.250</td>\n",
              "      <td>284.168</td>\n",
              "      <td>205.932</td>\n",
              "      <td>275.020</td>\n",
              "      <td>251.551</td>\n",
              "      <td>265.923</td>\n",
              "      <td>286.781</td>\n",
              "      <td>331.107</td>\n",
              "      <td>211.127</td>\n",
              "      <td>337.545</td>\n",
              "      <td>263.302</td>\n",
              "      <td>325.848</td>\n",
              "      <td>299.779</td>\n",
              "      <td>285.515</td>\n",
              "      <td>293.261</td>\n",
              "      <td>291.966</td>\n",
              "      <td>354.561</td>\n",
              "      <td>289.356</td>\n",
              "      <td>401.517</td>\n",
              "      <td>318.030</td>\n",
              "      <td>294.579</td>\n",
              "      <td>328.419</td>\n",
              "      <td>358.455</td>\n",
              "      <td>337.629</td>\n",
              "      <td>420.955</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>307.600</td>\n",
              "      <td>207.254</td>\n",
              "      <td>284.173</td>\n",
              "      <td>205.931</td>\n",
              "      <td>275.016</td>\n",
              "      <td>251.550</td>\n",
              "      <td>265.919</td>\n",
              "      <td>286.792</td>\n",
              "      <td>331.101</td>\n",
              "      <td>211.132</td>\n",
              "      <td>337.561</td>\n",
              "      <td>263.289</td>\n",
              "      <td>325.891</td>\n",
              "      <td>299.781</td>\n",
              "      <td>286.713</td>\n",
              "      <td>293.268</td>\n",
              "      <td>291.954</td>\n",
              "      <td>354.529</td>\n",
              "      <td>289.358</td>\n",
              "      <td>401.508</td>\n",
              "      <td>318.028</td>\n",
              "      <td>294.586</td>\n",
              "      <td>328.429</td>\n",
              "      <td>358.462</td>\n",
              "      <td>337.632</td>\n",
              "      <td>420.970</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         2        3        4        5  ...       25       26       27  y\n",
              "0  307.602  207.250  285.384  205.939  ...  358.453  337.620  419.732  2\n",
              "1  307.593  205.975  284.154  204.656  ...  358.463  337.624  420.964  2\n",
              "2  307.587  207.241  284.136  204.675  ...  358.453  337.624  420.957  2\n",
              "3  307.602  207.250  284.168  205.932  ...  358.455  337.629  420.955  2\n",
              "4  307.600  207.254  284.173  205.931  ...  358.462  337.632  420.970  2\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJoIxNckvz4j"
      },
      "source": [
        "### (2) Extract Angle from Joint (x,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yznX20nawL_9"
      },
      "source": [
        "#### - Extract Angle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNC9sH3rwkaQ"
      },
      "source": [
        "Joints\n",
        "\n",
        "```\n",
        "{1,  \"Neck\"}, -> '2', '3'\n",
        "{2,  \"RShoulder\"}, -> '4', '5'\n",
        "{3,  \"RElbow\"}, -> '6', '7'\n",
        "{4,  \"RWrist\"}, -> '8', '9'\n",
        "{5,  \"LShoulder\"}, -> '10', '11'\n",
        "{6,  \"LElbow\"}, -> '12', '13'\n",
        "{7,  \"LWrist\"}, -> '14', '15'\n",
        "{8,  \"RHip\"}, -> '16', '17'\n",
        "{9,  \"RKnee\"}, -> '18', '19'\n",
        "{10, \"RAnkle\"}, -> '20', '21'\n",
        "{11, \"LHip\"}, -> '22', '23'\n",
        "{12, \"LKnee\"}, -> '24', '25'\n",
        "{13, \"LAnkle\"}, -> '26', '27'\n",
        "```\n",
        "\n",
        "Angle\n",
        "\n",
        "```\n",
        "Angle__RShoulder : Neck - RShoulder - RElbow\n",
        "Angle_RElbow : RShoulder - RElbow - RWrist\n",
        "Angle_RHip : Neck - RHip - RKnee\n",
        "Angle_RKnee : RHip - RKnee - RAnkle\n",
        "\n",
        "Angle__LShoulder : Neck - LShoulder - LElbow\n",
        "Angle_LElbow : LShoulder - LElbow - LWrist\n",
        "Angle_LHip : Neck - LHip - LKnee\n",
        "Angle_LKnee : LHip - LKnee - LAnkle\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWibAyN_whpX"
      },
      "source": [
        "def makeJointXY(df):\n",
        "    df['Neck'] = [np.array(list(i)) for i in tqdm(zip(df['2'], df['3']))]\n",
        "\n",
        "    df['RShoulder'] = [np.array(list(i)) for i in tqdm(zip(df['4'], df['5']))]\n",
        "    df['RElbow'] = [np.array(list(i)) for i in tqdm(zip(df['6'], df['7']))]\n",
        "    df['RWrist'] = [np.array(list(i)) for i in tqdm(zip(df['8'], df['9']))]\n",
        "    df['RHip'] = [np.array(list(i)) for i in tqdm(zip(df['16'], df['17']))]\n",
        "    df['RKnee'] = [np.array(list(i)) for i in tqdm(zip(df['18'], df['19']))]\n",
        "    df['RAnkle'] = [np.array(list(i)) for i in tqdm(zip(df['20'], df['21']))]\n",
        "\n",
        "    df['LShoulder'] = [np.array(list(i)) for i in tqdm(zip(df['10'], df['11']))]\n",
        "    df['LElbow'] = [np.array(list(i)) for i in tqdm(zip(df['12'], df['13']))]\n",
        "    df['LWrist'] = [np.array(list(i)) for i in tqdm(zip(df['14'], df['15']))]\n",
        "    df['LHip'] = [np.array(list(i)) for i in tqdm(zip(df['22'], df['23']))]\n",
        "    df['LKnee'] = [np.array(list(i)) for i in tqdm(zip(df['24'], df['25']))]\n",
        "    df['LAnkle'] = [np.array(list(i)) for i in tqdm(zip(df['26'], df['27']))]\n",
        "\n",
        "    df.drop([str(i) for i in range(2, 28)], axis=1, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d8zf6k77-Ub"
      },
      "source": [
        "def extractAngle(a, joint, c):\n",
        "    a = np.array(a)\n",
        "    joint = np.array(joint)\n",
        "    c = np.array(c)\n",
        "    \n",
        "    a_j = a - joint\n",
        "    c_j = c - joint\n",
        "\n",
        "    theta = []\n",
        "\n",
        "    for i in tqdm(range(a.shape[0])):\n",
        "        th_a_j = math.atan2(a_j[i][1], a_j[i][0])\n",
        "        th_c_j = math.atan2(c_j[i][1], c_j[i][0])\n",
        "        theta.append(th_a_j - th_c_j)\n",
        "\n",
        "    return theta"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFLOecwCv1TT"
      },
      "source": [
        "def jointAngles(df):\n",
        "    df['Ang_RShoulder'] = extractAngle(df['Neck'], df['RShoulder'], df['RElbow'])\n",
        "    df['Ang_RElbow'] = extractAngle(df['RShoulder'], df['RElbow'], df['RWrist'])\n",
        "    df['Ang_RHip'] = extractAngle(df['Neck'], df['RHip'], df['RKnee'])\n",
        "    df['Ang_RKnee'] = extractAngle(df['RHip'], df['RKnee'], df['RAnkle'])\n",
        "\n",
        "    df['Ang_LShoulder'] = extractAngle(df['Neck'], df['LShoulder'], df['LElbow'])\n",
        "    df['Ang_LElbow'] = extractAngle(df['LShoulder'], df['LElbow'], df['LWrist'])\n",
        "    df['Ang_LHip'] = extractAngle(df['Neck'], df['LHip'], df['LKnee'])\n",
        "    df['Ang_LKnee'] = extractAngle(df['LHip'], df['LKnee'], df['LAnkle'])\n",
        "    \n",
        "    df.drop(df.columns[1:14], axis=1, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5p6jNfmVgKC",
        "outputId": "a159b109-4e14-45c6-c302-9a65e1cfb72d"
      },
      "source": [
        "df = makeJointXY(df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7735616it [00:17, 454963.84it/s]\n",
            "7735616it [00:16, 470750.14it/s]\n",
            "7735616it [00:16, 481648.94it/s]\n",
            "7735616it [00:16, 474801.33it/s]\n",
            "7735616it [00:16, 476590.86it/s]\n",
            "7735616it [00:19, 397279.14it/s]\n",
            "7735616it [00:16, 467196.90it/s]\n",
            "7735616it [00:16, 468480.44it/s]\n",
            "7735616it [00:16, 476582.63it/s]\n",
            "7735616it [00:16, 476979.64it/s]\n",
            "7735616it [00:16, 459354.04it/s]\n",
            "7735616it [00:16, 464357.78it/s]\n",
            "7735616it [00:16, 475576.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_S7OVpihmFQ",
        "outputId": "3f2becbd-a9ea-47eb-9a6a-8968509ad636"
      },
      "source": [
        "df = jointAngles(df)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7735616/7735616 [00:14<00:00, 548810.98it/s]\n",
            "100%|██████████| 7735616/7735616 [00:14<00:00, 550191.04it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 552661.87it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 552618.54it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 554035.38it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 560444.20it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 562667.53it/s]\n",
            "100%|██████████| 7735616/7735616 [00:13<00:00, 557923.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XEofd5q62bT"
      },
      "source": [
        "## 5) Make 'y'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTNRjbhpJ4q"
      },
      "source": [
        "def makeY(df, n_frame=32):\n",
        "    y = df['y'].to_list()\n",
        "    y = [y[i * n_frame:(i + 1) * n_frame] for i in range((len(y) + n_frame - 1) // n_frame )]\n",
        "    y = [ys[0] for ys in y]\n",
        "\n",
        "    encoder = LabelEncoder()    \n",
        "    encoder.fit(y)\n",
        "    classes = encoder.classes_\n",
        "    y = encoder.transform(y)\n",
        "\n",
        "    y = to_categorical(y)\n",
        "\n",
        "    return y, classes"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-plaqsxHUmj_"
      },
      "source": [
        "y, classes = makeY(df)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj1pU3y3Vf9n",
        "outputId": "2c3398a4-64c7-4e70-a2ad-ff1fd3969832"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(241738, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUI83OYlarQg",
        "outputId": "0fe6c757-b325-4416-87fd-5341e39a48ee"
      },
      "source": [
        "classes"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "helgUckxQoA2",
        "outputId": "ebb087ff-956c-4175-e292-afc6ed7d30e1"
      },
      "source": [
        "y"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVd-N0ln7C5T"
      },
      "source": [
        "## 6) Make 'X'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_RBSw3gF-o"
      },
      "source": [
        "def makeX(df, n_frame=32):\n",
        "    x_cat = df.iloc[:,1:].values\n",
        "\n",
        "    data = []\n",
        "    for rows in tqdm(x_cat):\n",
        "        for element in rows:\n",
        "            data.append(np.float32(element))\n",
        "\n",
        "    x = np.array(data)\n",
        "    x = x.reshape(-1, 32, 8)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixK4Trj8VZGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ddb8ab-a3a8-498f-bd25-e7c9bfa404a2"
      },
      "source": [
        "X = makeX(df)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7735616/7735616 [01:12<00:00, 106056.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDD7HngbX4sl",
        "outputId": "e4032fdf-44f4-4e7d-ad8b-28f79850257d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(241738, 32, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4_fHZh-7LDl"
      },
      "source": [
        "## 7) Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwoxLdCjsgvD"
      },
      "source": [
        "random_seed = 0\n",
        "n_frame = 32"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oErPJ9Xxln6u"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_seed, stratify=y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=random_seed, stratify=y_train)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkoxTHK8jBj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e50fc928-0c9f-4365-c87d-9c01b52c5b2e"
      },
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((174051, 32, 8), (174051, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3SReDThx8ib",
        "outputId": "92bb566c-0c02-4d10-9570-803572d5091b"
      },
      "source": [
        "X_valid.shape, y_valid.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((43513, 32, 8), (43513, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NovA26_Mx8qJ",
        "outputId": "eca7ee2b-8b8d-4f57-e71b-d4a323f532b7"
      },
      "source": [
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((24174, 32, 8), (24174, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K5fXeUoqBr6",
        "outputId": "f3c0ad8d-961a-4fb8-885d-bb922e5a2ca3"
      },
      "source": [
        "y_t = np.argmax(y_train,axis=1).reshape(-1,1)\n",
        "y_t\n",
        "\n",
        "unique, counts = np.unique(y_t, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 50284, 1: 48176, 2: 4277, 3: 64502, 4: 3606, 5: 3206}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFl18u8DsVae",
        "outputId": "e690c01d-4c25-4459-f837-22c69f228d85"
      },
      "source": [
        "y_v = np.argmax(y_valid,axis=1).reshape(-1,1)\n",
        "y_v\n",
        "\n",
        "unique, counts = np.unique(y_v, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 12571, 1: 12044, 2: 1069, 3: 16126, 4: 902, 5: 801}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFPuxKwOsXsg",
        "outputId": "1998df21-ee15-4cb5-d96b-1e74f68289b1"
      },
      "source": [
        "y_te = np.argmax(y_test,axis=1).reshape(-1,1)\n",
        "y_te\n",
        "\n",
        "unique, counts = np.unique(y_te, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 6984, 1: 6691, 2: 594, 3: 8959, 4: 501, 5: 445}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhrXnYoXd3n"
      },
      "source": [
        "### 8) Save Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxZO3BXXr2m"
      },
      "source": [
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_train.npy', X_train)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_valid.npy', X_valid)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_test.npy', X_test)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_train.npy', y_train)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_valid.npy', y_valid)\n",
        "np.save('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_test.npy', y_test)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF7jVbTkWpDw"
      },
      "source": [
        "# 2. LSTM Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MmjG0qCYU_l"
      },
      "source": [
        "## 0) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJbI-5-7YWyt"
      },
      "source": [
        "X_train = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_train.npy')\n",
        "X_valid = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_valid.npy')\n",
        "X_test = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/X_test.npy')\n",
        "\n",
        "y_train = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_train.npy')\n",
        "y_valid = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_valid.npy')\n",
        "y_test = np.load('/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/y_test.npy')\n"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_GPIteQ_FVm"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "import keras.backend as K"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oESk5HdJ_1CK"
      },
      "source": [
        "n_input = 8 # angles\n",
        "n_steps = 32 # per frames\n",
        "\n",
        "n_hidden = 32\n",
        "n_classes = len(y[0])\n",
        "\n",
        "epochs = 1000\n",
        "batch_size = 16384\n",
        "\n",
        "lambda_loss_amount = 0.0015"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609oD-_Wsbd"
      },
      "source": [
        "## 1) Model define"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DxO7PrMWuCR"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(n_hidden,\n",
        "                activation='relu',\n",
        "                bias_initializer='random_normal', # set initial bias by normal distribution\n",
        "                input_shape=(n_steps, n_input)))\n",
        "model.add(LSTM(n_hidden,\n",
        "               return_sequences=True, # return sequences\n",
        "               unit_forget_bias=1.0)) # bias_initializer=\"zeros\"\n",
        "model.add(LSTM(n_hidden,\n",
        "               unit_forget_bias=1.0)) # bias_initializer=\"zeros\"\n",
        "\n",
        "model.add(Dense(n_classes,\n",
        "                kernel_initializer='random_normal', # set initial weights by normal distribution\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(lambda_loss_amount), # weight regularizer\n",
        "                bias_regularizer=tf.keras.regularizers.l2(lambda_loss_amount), # bias regularizer\n",
        "                activation = 'softmax'))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNGSpLQoWyM8",
        "outputId": "15198b35-9302-4c9a-e36c-77f8dcb160d5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 32, 32)            288       \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 32, 32)            8320      \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                8320      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 198       \n",
            "=================================================================\n",
            "Total params: 17,126\n",
            "Trainable params: 17,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901bRI1DWy2G"
      },
      "source": [
        "## 2) Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNSl7h72W3Z8"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvbHXQ4hW0Ps"
      },
      "source": [
        "## 3) Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVatnxqSW85b",
        "outputId": "b1cd9e6d-4113-4106-dca9-7e111b462b3c"
      },
      "source": [
        "%%time\n",
        "hist = model.fit(X_train, y_train,\n",
        "                 epochs = epochs,\n",
        "                 batch_size = batch_size,\n",
        "                 validation_data = (X_valid, y_valid))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 25s 204ms/step - loss: 1.7513 - accuracy: 0.2406 - val_loss: 1.6117 - val_accuracy: 0.2766\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.5655 - accuracy: 0.2769 - val_loss: 1.4444 - val_accuracy: 0.4045\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.4185 - accuracy: 0.3864 - val_loss: 1.3557 - val_accuracy: 0.3706\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.3447 - accuracy: 0.3692 - val_loss: 1.3115 - val_accuracy: 0.3706\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.3045 - accuracy: 0.3711 - val_loss: 1.2856 - val_accuracy: 0.4625\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.2792 - accuracy: 0.4660 - val_loss: 1.2620 - val_accuracy: 0.4736\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.2548 - accuracy: 0.4896 - val_loss: 1.2314 - val_accuracy: 0.5247\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.2235 - accuracy: 0.5342 - val_loss: 1.1910 - val_accuracy: 0.5805\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.1773 - accuracy: 0.6119 - val_loss: 1.1348 - val_accuracy: 0.6562\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.1171 - accuracy: 0.6567 - val_loss: 1.0744 - val_accuracy: 0.6589\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 1.0581 - accuracy: 0.6683 - val_loss: 1.0154 - val_accuracy: 0.6841\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 1.0024 - accuracy: 0.6875 - val_loss: 0.9752 - val_accuracy: 0.6789\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.9585 - accuracy: 0.6906 - val_loss: 0.9244 - val_accuracy: 0.7040\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.9103 - accuracy: 0.7086 - val_loss: 0.8761 - val_accuracy: 0.7230\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.8694 - accuracy: 0.7226 - val_loss: 0.8545 - val_accuracy: 0.7213\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.8370 - accuracy: 0.7271 - val_loss: 0.8103 - val_accuracy: 0.7392\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.8039 - accuracy: 0.7381 - val_loss: 0.7877 - val_accuracy: 0.7411\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.7854 - accuracy: 0.7426 - val_loss: 0.7659 - val_accuracy: 0.7498\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.7633 - accuracy: 0.7512 - val_loss: 0.7586 - val_accuracy: 0.7499\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.7471 - accuracy: 0.7562 - val_loss: 0.7321 - val_accuracy: 0.7625\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.7271 - accuracy: 0.7647 - val_loss: 0.7387 - val_accuracy: 0.7558\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.7185 - accuracy: 0.7671 - val_loss: 0.7081 - val_accuracy: 0.7699\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.6987 - accuracy: 0.7747 - val_loss: 0.6955 - val_accuracy: 0.7762\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.6877 - accuracy: 0.7780 - val_loss: 0.6878 - val_accuracy: 0.7768\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.6752 - accuracy: 0.7822 - val_loss: 0.6640 - val_accuracy: 0.7878\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.6564 - accuracy: 0.7895 - val_loss: 0.6557 - val_accuracy: 0.7893\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.6521 - accuracy: 0.7905 - val_loss: 0.6363 - val_accuracy: 0.7970\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.6348 - accuracy: 0.7954 - val_loss: 0.6747 - val_accuracy: 0.7773\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.6504 - accuracy: 0.7876 - val_loss: 0.6413 - val_accuracy: 0.7919\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.6291 - accuracy: 0.7965 - val_loss: 0.6071 - val_accuracy: 0.8059\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.6076 - accuracy: 0.8062 - val_loss: 0.6007 - val_accuracy: 0.8068\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.5994 - accuracy: 0.8067 - val_loss: 0.5849 - val_accuracy: 0.8128\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.5775 - accuracy: 0.8156 - val_loss: 0.5761 - val_accuracy: 0.8150\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.5761 - accuracy: 0.8149 - val_loss: 0.5633 - val_accuracy: 0.8199\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.5670 - accuracy: 0.8174 - val_loss: 0.5631 - val_accuracy: 0.8210\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.5525 - accuracy: 0.8229 - val_loss: 0.5460 - val_accuracy: 0.8236\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5417 - accuracy: 0.8258 - val_loss: 0.5458 - val_accuracy: 0.8226\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.5470 - accuracy: 0.8212 - val_loss: 0.5266 - val_accuracy: 0.8309\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5289 - accuracy: 0.8292 - val_loss: 0.5176 - val_accuracy: 0.8333\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.5173 - accuracy: 0.8323 - val_loss: 0.5046 - val_accuracy: 0.8385\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.5041 - accuracy: 0.8387 - val_loss: 0.5004 - val_accuracy: 0.8383\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.5158 - accuracy: 0.8318 - val_loss: 0.5025 - val_accuracy: 0.8378\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.4974 - accuracy: 0.8396 - val_loss: 0.4859 - val_accuracy: 0.8432\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4878 - accuracy: 0.8429 - val_loss: 0.4833 - val_accuracy: 0.8430\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4799 - accuracy: 0.8449 - val_loss: 0.4703 - val_accuracy: 0.8471\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4711 - accuracy: 0.8475 - val_loss: 0.4652 - val_accuracy: 0.8494\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4657 - accuracy: 0.8489 - val_loss: 0.4760 - val_accuracy: 0.8451\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4698 - accuracy: 0.8478 - val_loss: 0.4532 - val_accuracy: 0.8529\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4538 - accuracy: 0.8538 - val_loss: 0.4482 - val_accuracy: 0.8522\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4484 - accuracy: 0.8543 - val_loss: 0.4476 - val_accuracy: 0.8520\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4428 - accuracy: 0.8566 - val_loss: 0.4324 - val_accuracy: 0.8571\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4404 - accuracy: 0.8562 - val_loss: 0.4605 - val_accuracy: 0.8459\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4405 - accuracy: 0.8552 - val_loss: 0.4237 - val_accuracy: 0.8601\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4224 - accuracy: 0.8623 - val_loss: 0.4166 - val_accuracy: 0.8628\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4243 - accuracy: 0.8614 - val_loss: 0.4460 - val_accuracy: 0.8507\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4294 - accuracy: 0.8580 - val_loss: 0.4134 - val_accuracy: 0.8641\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4110 - accuracy: 0.8660 - val_loss: 0.4053 - val_accuracy: 0.8683\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4041 - accuracy: 0.8691 - val_loss: 0.4211 - val_accuracy: 0.8621\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.4170 - accuracy: 0.8636 - val_loss: 0.4266 - val_accuracy: 0.8582\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4192 - accuracy: 0.8622 - val_loss: 0.4238 - val_accuracy: 0.8606\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4079 - accuracy: 0.8662 - val_loss: 0.4204 - val_accuracy: 0.8617\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.4107 - accuracy: 0.8655 - val_loss: 0.4043 - val_accuracy: 0.8679\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3974 - accuracy: 0.8717 - val_loss: 0.3870 - val_accuracy: 0.8748\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3814 - accuracy: 0.8765 - val_loss: 0.3839 - val_accuracy: 0.8760\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3870 - accuracy: 0.8747 - val_loss: 0.3799 - val_accuracy: 0.8778\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3870 - accuracy: 0.8748 - val_loss: 0.3817 - val_accuracy: 0.8767\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3760 - accuracy: 0.8792 - val_loss: 0.3713 - val_accuracy: 0.8792\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.3720 - accuracy: 0.8798 - val_loss: 0.3756 - val_accuracy: 0.8773\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3720 - accuracy: 0.8800 - val_loss: 0.3665 - val_accuracy: 0.8808\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3641 - accuracy: 0.8823 - val_loss: 0.4044 - val_accuracy: 0.8627\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3797 - accuracy: 0.8747 - val_loss: 0.3607 - val_accuracy: 0.8836\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3607 - accuracy: 0.8839 - val_loss: 0.3654 - val_accuracy: 0.8797\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3597 - accuracy: 0.8834 - val_loss: 0.3712 - val_accuracy: 0.8769\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3679 - accuracy: 0.8800 - val_loss: 0.3586 - val_accuracy: 0.8827\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3530 - accuracy: 0.8862 - val_loss: 0.3533 - val_accuracy: 0.8850\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3515 - accuracy: 0.8867 - val_loss: 0.3525 - val_accuracy: 0.8844\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3539 - accuracy: 0.8850 - val_loss: 0.3455 - val_accuracy: 0.8885\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3535 - accuracy: 0.8860 - val_loss: 0.3519 - val_accuracy: 0.8869\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3464 - accuracy: 0.8885 - val_loss: 0.3525 - val_accuracy: 0.8836\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3406 - accuracy: 0.8902 - val_loss: 0.3415 - val_accuracy: 0.8890\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3365 - accuracy: 0.8918 - val_loss: 0.3625 - val_accuracy: 0.8768\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3452 - accuracy: 0.8877 - val_loss: 0.3396 - val_accuracy: 0.8922\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3416 - accuracy: 0.8897 - val_loss: 0.3588 - val_accuracy: 0.8832\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3413 - accuracy: 0.8897 - val_loss: 0.3325 - val_accuracy: 0.8941\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3287 - accuracy: 0.8951 - val_loss: 0.3306 - val_accuracy: 0.8945\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3218 - accuracy: 0.8980 - val_loss: 0.3284 - val_accuracy: 0.8946\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3208 - accuracy: 0.8981 - val_loss: 0.3341 - val_accuracy: 0.8901\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3253 - accuracy: 0.8959 - val_loss: 0.3452 - val_accuracy: 0.8854\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3337 - accuracy: 0.8926 - val_loss: 0.3369 - val_accuracy: 0.8894\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3241 - accuracy: 0.8965 - val_loss: 0.3258 - val_accuracy: 0.8951\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3207 - accuracy: 0.8970 - val_loss: 0.3211 - val_accuracy: 0.8982\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3244 - accuracy: 0.8954 - val_loss: 0.3485 - val_accuracy: 0.8855\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3376 - accuracy: 0.8887 - val_loss: 0.3503 - val_accuracy: 0.8858\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3234 - accuracy: 0.8956 - val_loss: 0.3178 - val_accuracy: 0.8988\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3061 - accuracy: 0.9034 - val_loss: 0.3112 - val_accuracy: 0.9014\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.3027 - accuracy: 0.9046 - val_loss: 0.3077 - val_accuracy: 0.9032\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3011 - accuracy: 0.9048 - val_loss: 0.3120 - val_accuracy: 0.9021\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3158 - accuracy: 0.8980 - val_loss: 0.3363 - val_accuracy: 0.8870\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3178 - accuracy: 0.8965 - val_loss: 0.3210 - val_accuracy: 0.8972\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.3049 - accuracy: 0.9024 - val_loss: 0.3055 - val_accuracy: 0.9025\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2962 - accuracy: 0.9063 - val_loss: 0.3087 - val_accuracy: 0.9024\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2994 - accuracy: 0.9047 - val_loss: 0.3069 - val_accuracy: 0.9014\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2965 - accuracy: 0.9060 - val_loss: 0.3030 - val_accuracy: 0.9034\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2916 - accuracy: 0.9077 - val_loss: 0.3071 - val_accuracy: 0.9001\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3100 - accuracy: 0.8998 - val_loss: 0.3349 - val_accuracy: 0.8898\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.3128 - accuracy: 0.8985 - val_loss: 0.3020 - val_accuracy: 0.9041\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2909 - accuracy: 0.9082 - val_loss: 0.2956 - val_accuracy: 0.9061\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2927 - accuracy: 0.9064 - val_loss: 0.3112 - val_accuracy: 0.8979\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2915 - accuracy: 0.9075 - val_loss: 0.2987 - val_accuracy: 0.9057\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2861 - accuracy: 0.9093 - val_loss: 0.2888 - val_accuracy: 0.9082\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2876 - accuracy: 0.9087 - val_loss: 0.2899 - val_accuracy: 0.9094\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2785 - accuracy: 0.9134 - val_loss: 0.2893 - val_accuracy: 0.9082\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2823 - accuracy: 0.9114 - val_loss: 0.2877 - val_accuracy: 0.9102\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2810 - accuracy: 0.9126 - val_loss: 0.2840 - val_accuracy: 0.9122\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2792 - accuracy: 0.9128 - val_loss: 0.2909 - val_accuracy: 0.9090\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2756 - accuracy: 0.9145 - val_loss: 0.2810 - val_accuracy: 0.9119\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2709 - accuracy: 0.9161 - val_loss: 0.3138 - val_accuracy: 0.8987\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2958 - accuracy: 0.9062 - val_loss: 0.3181 - val_accuracy: 0.8965\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2943 - accuracy: 0.9061 - val_loss: 0.2781 - val_accuracy: 0.9137\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2738 - accuracy: 0.9146 - val_loss: 0.2781 - val_accuracy: 0.9143\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2666 - accuracy: 0.9177 - val_loss: 0.2778 - val_accuracy: 0.9148\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.2681 - accuracy: 0.9177 - val_loss: 0.2857 - val_accuracy: 0.9104\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2718 - accuracy: 0.9154 - val_loss: 0.2827 - val_accuracy: 0.9114\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2716 - accuracy: 0.9150 - val_loss: 0.2788 - val_accuracy: 0.9131\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.2680 - accuracy: 0.9166 - val_loss: 0.2780 - val_accuracy: 0.9134\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2652 - accuracy: 0.9180 - val_loss: 0.2913 - val_accuracy: 0.9083\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2725 - accuracy: 0.9142 - val_loss: 0.2844 - val_accuracy: 0.9104\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2703 - accuracy: 0.9154 - val_loss: 0.2740 - val_accuracy: 0.9149\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2625 - accuracy: 0.9192 - val_loss: 0.2925 - val_accuracy: 0.9062\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2679 - accuracy: 0.9166 - val_loss: 0.2654 - val_accuracy: 0.9183\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2558 - accuracy: 0.9220 - val_loss: 0.2680 - val_accuracy: 0.9164\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2602 - accuracy: 0.9202 - val_loss: 0.2639 - val_accuracy: 0.9194\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2639 - accuracy: 0.9187 - val_loss: 0.2620 - val_accuracy: 0.9208\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2652 - accuracy: 0.9167 - val_loss: 0.2629 - val_accuracy: 0.9200\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2526 - accuracy: 0.9231 - val_loss: 0.2645 - val_accuracy: 0.9180\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2519 - accuracy: 0.9225 - val_loss: 0.2636 - val_accuracy: 0.9189\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2509 - accuracy: 0.9231 - val_loss: 0.2615 - val_accuracy: 0.9189\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2842 - accuracy: 0.9093 - val_loss: 0.2665 - val_accuracy: 0.9177\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2589 - accuracy: 0.9206 - val_loss: 0.2632 - val_accuracy: 0.9201\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2518 - accuracy: 0.9231 - val_loss: 0.2652 - val_accuracy: 0.9175\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2485 - accuracy: 0.9242 - val_loss: 0.2570 - val_accuracy: 0.9205\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2561 - accuracy: 0.9207 - val_loss: 0.2565 - val_accuracy: 0.9215\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2463 - accuracy: 0.9247 - val_loss: 0.2690 - val_accuracy: 0.9166\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2542 - accuracy: 0.9215 - val_loss: 0.2683 - val_accuracy: 0.9154\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2482 - accuracy: 0.9239 - val_loss: 0.2647 - val_accuracy: 0.9163\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2462 - accuracy: 0.9245 - val_loss: 0.2688 - val_accuracy: 0.9159\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2598 - accuracy: 0.9189 - val_loss: 0.2484 - val_accuracy: 0.9245\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2397 - accuracy: 0.9273 - val_loss: 0.2818 - val_accuracy: 0.9107\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2559 - accuracy: 0.9201 - val_loss: 0.2524 - val_accuracy: 0.9229\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2406 - accuracy: 0.9275 - val_loss: 0.2678 - val_accuracy: 0.9154\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2624 - accuracy: 0.9168 - val_loss: 0.2607 - val_accuracy: 0.9185\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2473 - accuracy: 0.9244 - val_loss: 0.2518 - val_accuracy: 0.9210\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2400 - accuracy: 0.9274 - val_loss: 0.2530 - val_accuracy: 0.9224\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2400 - accuracy: 0.9272 - val_loss: 0.2429 - val_accuracy: 0.9264\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2349 - accuracy: 0.9293 - val_loss: 0.2476 - val_accuracy: 0.9236\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2320 - accuracy: 0.9302 - val_loss: 0.2440 - val_accuracy: 0.9265\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2285 - accuracy: 0.9318 - val_loss: 0.2408 - val_accuracy: 0.9258\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2342 - accuracy: 0.9288 - val_loss: 0.2408 - val_accuracy: 0.9274\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2360 - accuracy: 0.9285 - val_loss: 0.2651 - val_accuracy: 0.9178\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2410 - accuracy: 0.9263 - val_loss: 0.2416 - val_accuracy: 0.9264\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2337 - accuracy: 0.9294 - val_loss: 0.2438 - val_accuracy: 0.9255\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2408 - accuracy: 0.9263 - val_loss: 0.2442 - val_accuracy: 0.9254\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2297 - accuracy: 0.9309 - val_loss: 0.2378 - val_accuracy: 0.9273\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2286 - accuracy: 0.9310 - val_loss: 0.2442 - val_accuracy: 0.9254\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2289 - accuracy: 0.9313 - val_loss: 0.2380 - val_accuracy: 0.9277\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2299 - accuracy: 0.9312 - val_loss: 0.2658 - val_accuracy: 0.9167\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2412 - accuracy: 0.9258 - val_loss: 0.2388 - val_accuracy: 0.9274\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2258 - accuracy: 0.9319 - val_loss: 0.2334 - val_accuracy: 0.9305\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2233 - accuracy: 0.9336 - val_loss: 0.2538 - val_accuracy: 0.9218\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2541 - accuracy: 0.9192 - val_loss: 0.2648 - val_accuracy: 0.9164\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2400 - accuracy: 0.9264 - val_loss: 0.2398 - val_accuracy: 0.9266\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2235 - accuracy: 0.9337 - val_loss: 0.2305 - val_accuracy: 0.9308\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2187 - accuracy: 0.9358 - val_loss: 0.2329 - val_accuracy: 0.9289\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2248 - accuracy: 0.9328 - val_loss: 0.2413 - val_accuracy: 0.9259\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2529 - accuracy: 0.9205 - val_loss: 0.2932 - val_accuracy: 0.9012\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2556 - accuracy: 0.9182 - val_loss: 0.2389 - val_accuracy: 0.9259\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2337 - accuracy: 0.9280 - val_loss: 0.2428 - val_accuracy: 0.9249\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2213 - accuracy: 0.9345 - val_loss: 0.2276 - val_accuracy: 0.9322\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2152 - accuracy: 0.9369 - val_loss: 0.2272 - val_accuracy: 0.9326\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2148 - accuracy: 0.9376 - val_loss: 0.2325 - val_accuracy: 0.9298\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2244 - accuracy: 0.9326 - val_loss: 0.2369 - val_accuracy: 0.9273\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2308 - accuracy: 0.9300 - val_loss: 0.2327 - val_accuracy: 0.9293\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2201 - accuracy: 0.9339 - val_loss: 0.2298 - val_accuracy: 0.9307\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2148 - accuracy: 0.9367 - val_loss: 0.2325 - val_accuracy: 0.9295\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2207 - accuracy: 0.9338 - val_loss: 0.2251 - val_accuracy: 0.9330\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2207 - accuracy: 0.9348 - val_loss: 0.2326 - val_accuracy: 0.9298\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2190 - accuracy: 0.9351 - val_loss: 0.2249 - val_accuracy: 0.9334\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2104 - accuracy: 0.9380 - val_loss: 0.2207 - val_accuracy: 0.9347\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2075 - accuracy: 0.9397 - val_loss: 0.2211 - val_accuracy: 0.9346\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2064 - accuracy: 0.9396 - val_loss: 0.2225 - val_accuracy: 0.9334\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2059 - accuracy: 0.9400 - val_loss: 0.2182 - val_accuracy: 0.9347\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2041 - accuracy: 0.9403 - val_loss: 0.2205 - val_accuracy: 0.9350\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2118 - accuracy: 0.9372 - val_loss: 0.2238 - val_accuracy: 0.9326\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2076 - accuracy: 0.9394 - val_loss: 0.2203 - val_accuracy: 0.9351\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2094 - accuracy: 0.9390 - val_loss: 0.2331 - val_accuracy: 0.9277\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2338 - accuracy: 0.9277 - val_loss: 0.2211 - val_accuracy: 0.9347\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2106 - accuracy: 0.9382 - val_loss: 0.2217 - val_accuracy: 0.9339\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2074 - accuracy: 0.9400 - val_loss: 0.2265 - val_accuracy: 0.9324\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2034 - accuracy: 0.9405 - val_loss: 0.2258 - val_accuracy: 0.9311\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2104 - accuracy: 0.9379 - val_loss: 0.2139 - val_accuracy: 0.9373\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2026 - accuracy: 0.9411 - val_loss: 0.2132 - val_accuracy: 0.9374\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2031 - accuracy: 0.9407 - val_loss: 0.2344 - val_accuracy: 0.9279\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2147 - accuracy: 0.9359 - val_loss: 0.2620 - val_accuracy: 0.9153\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.2307 - accuracy: 0.9291 - val_loss: 0.2403 - val_accuracy: 0.9253\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2214 - accuracy: 0.9329 - val_loss: 0.2140 - val_accuracy: 0.9360\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2003 - accuracy: 0.9425 - val_loss: 0.2091 - val_accuracy: 0.9391\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1983 - accuracy: 0.9434 - val_loss: 0.2126 - val_accuracy: 0.9373\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1986 - accuracy: 0.9419 - val_loss: 0.2115 - val_accuracy: 0.9374\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2053 - accuracy: 0.9394 - val_loss: 0.2088 - val_accuracy: 0.9387\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1976 - accuracy: 0.9429 - val_loss: 0.2320 - val_accuracy: 0.9277\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2099 - accuracy: 0.9375 - val_loss: 0.2079 - val_accuracy: 0.9395\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1955 - accuracy: 0.9446 - val_loss: 0.2104 - val_accuracy: 0.9378\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1978 - accuracy: 0.9423 - val_loss: 0.2095 - val_accuracy: 0.9386\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1948 - accuracy: 0.9443 - val_loss: 0.2243 - val_accuracy: 0.9310\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2023 - accuracy: 0.9406 - val_loss: 0.2101 - val_accuracy: 0.9384\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2000 - accuracy: 0.9417 - val_loss: 0.2224 - val_accuracy: 0.9335\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2126 - accuracy: 0.9360 - val_loss: 0.2449 - val_accuracy: 0.9246\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2157 - accuracy: 0.9347 - val_loss: 0.2117 - val_accuracy: 0.9372\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1973 - accuracy: 0.9426 - val_loss: 0.2088 - val_accuracy: 0.9381\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1929 - accuracy: 0.9448 - val_loss: 0.2409 - val_accuracy: 0.9265\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2173 - accuracy: 0.9345 - val_loss: 0.2019 - val_accuracy: 0.9417\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1912 - accuracy: 0.9464 - val_loss: 0.2034 - val_accuracy: 0.9405\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1966 - accuracy: 0.9428 - val_loss: 0.2325 - val_accuracy: 0.9282\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2106 - accuracy: 0.9361 - val_loss: 0.2226 - val_accuracy: 0.9318\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2013 - accuracy: 0.9411 - val_loss: 0.2179 - val_accuracy: 0.9344\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1984 - accuracy: 0.9424 - val_loss: 0.2036 - val_accuracy: 0.9408\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1900 - accuracy: 0.9458 - val_loss: 0.2040 - val_accuracy: 0.9397\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1891 - accuracy: 0.9457 - val_loss: 0.1993 - val_accuracy: 0.9430\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1872 - accuracy: 0.9471 - val_loss: 0.2304 - val_accuracy: 0.9294\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2023 - accuracy: 0.9404 - val_loss: 0.2072 - val_accuracy: 0.9395\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1914 - accuracy: 0.9451 - val_loss: 0.2146 - val_accuracy: 0.9350\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2058 - accuracy: 0.9385 - val_loss: 0.1994 - val_accuracy: 0.9431\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1892 - accuracy: 0.9464 - val_loss: 0.1985 - val_accuracy: 0.9427\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1920 - accuracy: 0.9451 - val_loss: 0.2117 - val_accuracy: 0.9364\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1952 - accuracy: 0.9433 - val_loss: 0.2094 - val_accuracy: 0.9374\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2004 - accuracy: 0.9409 - val_loss: 0.2042 - val_accuracy: 0.9400\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1861 - accuracy: 0.9473 - val_loss: 0.1954 - val_accuracy: 0.9437\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1820 - accuracy: 0.9492 - val_loss: 0.2148 - val_accuracy: 0.9354\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1939 - accuracy: 0.9433 - val_loss: 0.1993 - val_accuracy: 0.9422\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1827 - accuracy: 0.9487 - val_loss: 0.1962 - val_accuracy: 0.9429\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1799 - accuracy: 0.9490 - val_loss: 0.1962 - val_accuracy: 0.9430\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1847 - accuracy: 0.9476 - val_loss: 0.2141 - val_accuracy: 0.9344\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1929 - accuracy: 0.9440 - val_loss: 0.1936 - val_accuracy: 0.9439\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1873 - accuracy: 0.9467 - val_loss: 0.2150 - val_accuracy: 0.9349\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1886 - accuracy: 0.9456 - val_loss: 0.1927 - val_accuracy: 0.9440\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1784 - accuracy: 0.9505 - val_loss: 0.1945 - val_accuracy: 0.9437\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1860 - accuracy: 0.9466 - val_loss: 0.1987 - val_accuracy: 0.9422\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1879 - accuracy: 0.9458 - val_loss: 0.1988 - val_accuracy: 0.9426\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1831 - accuracy: 0.9483 - val_loss: 0.1917 - val_accuracy: 0.9444\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1932 - accuracy: 0.9431 - val_loss: 0.1926 - val_accuracy: 0.9445\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1896 - accuracy: 0.9454 - val_loss: 0.1906 - val_accuracy: 0.9457\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.1787 - accuracy: 0.9501 - val_loss: 0.1906 - val_accuracy: 0.9446\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1791 - accuracy: 0.9493 - val_loss: 0.1986 - val_accuracy: 0.9412\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1874 - accuracy: 0.9457 - val_loss: 0.1933 - val_accuracy: 0.9434\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1786 - accuracy: 0.9500 - val_loss: 0.1904 - val_accuracy: 0.9454\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1726 - accuracy: 0.9524 - val_loss: 0.1921 - val_accuracy: 0.9433\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1760 - accuracy: 0.9503 - val_loss: 0.1932 - val_accuracy: 0.9448\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1729 - accuracy: 0.9522 - val_loss: 0.1959 - val_accuracy: 0.9423\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1816 - accuracy: 0.9481 - val_loss: 0.1943 - val_accuracy: 0.9431\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1876 - accuracy: 0.9457 - val_loss: 0.1908 - val_accuracy: 0.9456\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1738 - accuracy: 0.9517 - val_loss: 0.1867 - val_accuracy: 0.9465\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1721 - accuracy: 0.9525 - val_loss: 0.2079 - val_accuracy: 0.9382\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2249 - accuracy: 0.9298 - val_loss: 0.1928 - val_accuracy: 0.9453\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1907 - accuracy: 0.9452 - val_loss: 0.1952 - val_accuracy: 0.9416\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1823 - accuracy: 0.9486 - val_loss: 0.1859 - val_accuracy: 0.9466\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1723 - accuracy: 0.9517 - val_loss: 0.1870 - val_accuracy: 0.9456\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1722 - accuracy: 0.9520 - val_loss: 0.1833 - val_accuracy: 0.9483\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1759 - accuracy: 0.9502 - val_loss: 0.1861 - val_accuracy: 0.9466\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1726 - accuracy: 0.9513 - val_loss: 0.1876 - val_accuracy: 0.9453\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1718 - accuracy: 0.9517 - val_loss: 0.1837 - val_accuracy: 0.9471\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1869 - accuracy: 0.9457 - val_loss: 0.2003 - val_accuracy: 0.9411\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1796 - accuracy: 0.9481 - val_loss: 0.1824 - val_accuracy: 0.9488\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1669 - accuracy: 0.9538 - val_loss: 0.1806 - val_accuracy: 0.9492\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1658 - accuracy: 0.9546 - val_loss: 0.1827 - val_accuracy: 0.9474\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1707 - accuracy: 0.9525 - val_loss: 0.1825 - val_accuracy: 0.9486\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1736 - accuracy: 0.9511 - val_loss: 0.1799 - val_accuracy: 0.9494\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1668 - accuracy: 0.9542 - val_loss: 0.2432 - val_accuracy: 0.9250\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1980 - accuracy: 0.9411 - val_loss: 0.1851 - val_accuracy: 0.9468\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1696 - accuracy: 0.9528 - val_loss: 0.1795 - val_accuracy: 0.9492\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1641 - accuracy: 0.9549 - val_loss: 0.1812 - val_accuracy: 0.9479\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1664 - accuracy: 0.9538 - val_loss: 0.1842 - val_accuracy: 0.9461\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1676 - accuracy: 0.9529 - val_loss: 0.1926 - val_accuracy: 0.9436\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1821 - accuracy: 0.9468 - val_loss: 0.1823 - val_accuracy: 0.9475\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1728 - accuracy: 0.9518 - val_loss: 0.1770 - val_accuracy: 0.9496\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1640 - accuracy: 0.9547 - val_loss: 0.1767 - val_accuracy: 0.9495\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1689 - accuracy: 0.9528 - val_loss: 0.1759 - val_accuracy: 0.9504\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1606 - accuracy: 0.9563 - val_loss: 0.1759 - val_accuracy: 0.9505\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1625 - accuracy: 0.9558 - val_loss: 0.2433 - val_accuracy: 0.9243\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.2514 - accuracy: 0.9211 - val_loss: 0.1971 - val_accuracy: 0.9421\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1934 - accuracy: 0.9428 - val_loss: 0.1932 - val_accuracy: 0.9434\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1775 - accuracy: 0.9489 - val_loss: 0.1827 - val_accuracy: 0.9473\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1651 - accuracy: 0.9543 - val_loss: 0.1776 - val_accuracy: 0.9497\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1615 - accuracy: 0.9559 - val_loss: 0.1822 - val_accuracy: 0.9483\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1655 - accuracy: 0.9541 - val_loss: 0.1786 - val_accuracy: 0.9496\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1594 - accuracy: 0.9573 - val_loss: 0.1723 - val_accuracy: 0.9515\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1568 - accuracy: 0.9574 - val_loss: 0.1740 - val_accuracy: 0.9512\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1579 - accuracy: 0.9572 - val_loss: 0.1762 - val_accuracy: 0.9511\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1585 - accuracy: 0.9568 - val_loss: 0.1743 - val_accuracy: 0.9504\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1615 - accuracy: 0.9553 - val_loss: 0.2116 - val_accuracy: 0.9341\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1860 - accuracy: 0.9450 - val_loss: 0.2163 - val_accuracy: 0.9322\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1814 - accuracy: 0.9469 - val_loss: 0.1768 - val_accuracy: 0.9506\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1665 - accuracy: 0.9538 - val_loss: 0.1715 - val_accuracy: 0.9525\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1611 - accuracy: 0.9563 - val_loss: 0.1735 - val_accuracy: 0.9511\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1597 - accuracy: 0.9561 - val_loss: 0.1720 - val_accuracy: 0.9515\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1545 - accuracy: 0.9579 - val_loss: 0.1684 - val_accuracy: 0.9530\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1551 - accuracy: 0.9581 - val_loss: 0.1682 - val_accuracy: 0.9535\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1560 - accuracy: 0.9575 - val_loss: 0.1710 - val_accuracy: 0.9530\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1537 - accuracy: 0.9590 - val_loss: 0.1790 - val_accuracy: 0.9475\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1605 - accuracy: 0.9554 - val_loss: 0.1698 - val_accuracy: 0.9525\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1603 - accuracy: 0.9557 - val_loss: 0.2001 - val_accuracy: 0.9394\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1781 - accuracy: 0.9484 - val_loss: 0.1693 - val_accuracy: 0.9529\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1624 - accuracy: 0.9554 - val_loss: 0.1924 - val_accuracy: 0.9428\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1701 - accuracy: 0.9515 - val_loss: 0.1754 - val_accuracy: 0.9500\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1643 - accuracy: 0.9543 - val_loss: 0.1795 - val_accuracy: 0.9479\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1588 - accuracy: 0.9564 - val_loss: 0.1648 - val_accuracy: 0.9551\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1500 - accuracy: 0.9601 - val_loss: 0.1694 - val_accuracy: 0.9515\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1504 - accuracy: 0.9600 - val_loss: 0.1724 - val_accuracy: 0.9496\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1552 - accuracy: 0.9574 - val_loss: 0.2244 - val_accuracy: 0.9307\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1847 - accuracy: 0.9449 - val_loss: 0.2190 - val_accuracy: 0.9312\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1798 - accuracy: 0.9478 - val_loss: 0.1809 - val_accuracy: 0.9485\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1577 - accuracy: 0.9574 - val_loss: 0.1741 - val_accuracy: 0.9504\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1545 - accuracy: 0.9580 - val_loss: 0.1685 - val_accuracy: 0.9522\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1496 - accuracy: 0.9602 - val_loss: 0.1729 - val_accuracy: 0.9508\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1564 - accuracy: 0.9568 - val_loss: 0.2007 - val_accuracy: 0.9385\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1646 - accuracy: 0.9533 - val_loss: 0.1673 - val_accuracy: 0.9536\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1518 - accuracy: 0.9595 - val_loss: 0.1680 - val_accuracy: 0.9532\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1495 - accuracy: 0.9598 - val_loss: 0.1668 - val_accuracy: 0.9525\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1464 - accuracy: 0.9611 - val_loss: 0.1686 - val_accuracy: 0.9529\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1496 - accuracy: 0.9597 - val_loss: 0.1628 - val_accuracy: 0.9550\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1465 - accuracy: 0.9619 - val_loss: 0.1629 - val_accuracy: 0.9549\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1489 - accuracy: 0.9604 - val_loss: 0.1606 - val_accuracy: 0.9565\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1451 - accuracy: 0.9615 - val_loss: 0.1658 - val_accuracy: 0.9536\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1537 - accuracy: 0.9582 - val_loss: 0.1703 - val_accuracy: 0.9529\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1647 - accuracy: 0.9536 - val_loss: 0.1665 - val_accuracy: 0.9539\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1532 - accuracy: 0.9586 - val_loss: 0.1642 - val_accuracy: 0.9545\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1470 - accuracy: 0.9603 - val_loss: 0.1639 - val_accuracy: 0.9545\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1480 - accuracy: 0.9603 - val_loss: 0.1724 - val_accuracy: 0.9506\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1530 - accuracy: 0.9581 - val_loss: 0.1815 - val_accuracy: 0.9468\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.1588 - accuracy: 0.9558 - val_loss: 0.1923 - val_accuracy: 0.9420\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1642 - accuracy: 0.9534 - val_loss: 0.1679 - val_accuracy: 0.9528\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1496 - accuracy: 0.9595 - val_loss: 0.1650 - val_accuracy: 0.9534\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1463 - accuracy: 0.9607 - val_loss: 0.1582 - val_accuracy: 0.9568\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1458 - accuracy: 0.9611 - val_loss: 0.2207 - val_accuracy: 0.9327\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1809 - accuracy: 0.9472 - val_loss: 0.1650 - val_accuracy: 0.9541\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1482 - accuracy: 0.9599 - val_loss: 0.1622 - val_accuracy: 0.9545\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1461 - accuracy: 0.9604 - val_loss: 0.1582 - val_accuracy: 0.9577\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1574 - accuracy: 0.9565 - val_loss: 0.1760 - val_accuracy: 0.9501\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1624 - accuracy: 0.9542 - val_loss: 0.1684 - val_accuracy: 0.9510\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1552 - accuracy: 0.9569 - val_loss: 0.1638 - val_accuracy: 0.9554\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1453 - accuracy: 0.9615 - val_loss: 0.1621 - val_accuracy: 0.9551\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1533 - accuracy: 0.9581 - val_loss: 0.1756 - val_accuracy: 0.9490\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1509 - accuracy: 0.9594 - val_loss: 0.1600 - val_accuracy: 0.9566\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1496 - accuracy: 0.9594 - val_loss: 0.1623 - val_accuracy: 0.9546\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1432 - accuracy: 0.9616 - val_loss: 0.1551 - val_accuracy: 0.9582\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1499 - accuracy: 0.9595 - val_loss: 0.1648 - val_accuracy: 0.9545\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1476 - accuracy: 0.9603 - val_loss: 0.1561 - val_accuracy: 0.9581\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1449 - accuracy: 0.9613 - val_loss: 0.1603 - val_accuracy: 0.9560\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1526 - accuracy: 0.9580 - val_loss: 0.1682 - val_accuracy: 0.9532\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1535 - accuracy: 0.9576 - val_loss: 0.1687 - val_accuracy: 0.9527\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1457 - accuracy: 0.9607 - val_loss: 0.1528 - val_accuracy: 0.9589\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1401 - accuracy: 0.9634 - val_loss: 0.1543 - val_accuracy: 0.9581\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1372 - accuracy: 0.9646 - val_loss: 0.1517 - val_accuracy: 0.9591\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1371 - accuracy: 0.9650 - val_loss: 0.1541 - val_accuracy: 0.9589\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1394 - accuracy: 0.9639 - val_loss: 0.1704 - val_accuracy: 0.9509\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1473 - accuracy: 0.9600 - val_loss: 0.1590 - val_accuracy: 0.9555\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1401 - accuracy: 0.9632 - val_loss: 0.1556 - val_accuracy: 0.9576\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1403 - accuracy: 0.9633 - val_loss: 0.1750 - val_accuracy: 0.9502\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1465 - accuracy: 0.9608 - val_loss: 0.1583 - val_accuracy: 0.9563\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1412 - accuracy: 0.9623 - val_loss: 0.1547 - val_accuracy: 0.9588\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1439 - accuracy: 0.9617 - val_loss: 0.1573 - val_accuracy: 0.9565\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1510 - accuracy: 0.9585 - val_loss: 0.1525 - val_accuracy: 0.9589\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1403 - accuracy: 0.9632 - val_loss: 0.1675 - val_accuracy: 0.9524\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1421 - accuracy: 0.9614 - val_loss: 0.1633 - val_accuracy: 0.9540\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1410 - accuracy: 0.9621 - val_loss: 0.1562 - val_accuracy: 0.9570\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1432 - accuracy: 0.9613 - val_loss: 0.1617 - val_accuracy: 0.9554\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1405 - accuracy: 0.9628 - val_loss: 0.1677 - val_accuracy: 0.9527\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1440 - accuracy: 0.9614 - val_loss: 0.1532 - val_accuracy: 0.9584\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1429 - accuracy: 0.9619 - val_loss: 0.1539 - val_accuracy: 0.9576\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1385 - accuracy: 0.9640 - val_loss: 0.1501 - val_accuracy: 0.9603\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1363 - accuracy: 0.9640 - val_loss: 0.1628 - val_accuracy: 0.9545\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1500 - accuracy: 0.9592 - val_loss: 0.1754 - val_accuracy: 0.9489\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1517 - accuracy: 0.9580 - val_loss: 0.1637 - val_accuracy: 0.9540\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1462 - accuracy: 0.9605 - val_loss: 0.1551 - val_accuracy: 0.9580\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1350 - accuracy: 0.9654 - val_loss: 0.1527 - val_accuracy: 0.9588\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1328 - accuracy: 0.9662 - val_loss: 0.1550 - val_accuracy: 0.9575\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1396 - accuracy: 0.9630 - val_loss: 0.1521 - val_accuracy: 0.9587\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1475 - accuracy: 0.9595 - val_loss: 0.1565 - val_accuracy: 0.9565\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1398 - accuracy: 0.9631 - val_loss: 0.1517 - val_accuracy: 0.9595\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1416 - accuracy: 0.9618 - val_loss: 0.1551 - val_accuracy: 0.9573\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1448 - accuracy: 0.9613 - val_loss: 0.1468 - val_accuracy: 0.9611\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1346 - accuracy: 0.9650 - val_loss: 0.1497 - val_accuracy: 0.9601\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1339 - accuracy: 0.9657 - val_loss: 0.1512 - val_accuracy: 0.9597\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1340 - accuracy: 0.9660 - val_loss: 0.1585 - val_accuracy: 0.9561\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1385 - accuracy: 0.9635 - val_loss: 0.1497 - val_accuracy: 0.9601\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1353 - accuracy: 0.9651 - val_loss: 0.1485 - val_accuracy: 0.9598\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1346 - accuracy: 0.9653 - val_loss: 0.1602 - val_accuracy: 0.9550\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1356 - accuracy: 0.9644 - val_loss: 0.1483 - val_accuracy: 0.9603\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1359 - accuracy: 0.9641 - val_loss: 0.1480 - val_accuracy: 0.9598\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1338 - accuracy: 0.9655 - val_loss: 0.1516 - val_accuracy: 0.9591\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1382 - accuracy: 0.9638 - val_loss: 0.1454 - val_accuracy: 0.9622\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1405 - accuracy: 0.9625 - val_loss: 0.1534 - val_accuracy: 0.9587\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1315 - accuracy: 0.9666 - val_loss: 0.1456 - val_accuracy: 0.9615\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1286 - accuracy: 0.9677 - val_loss: 0.1445 - val_accuracy: 0.9613\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1291 - accuracy: 0.9677 - val_loss: 0.1459 - val_accuracy: 0.9604\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1272 - accuracy: 0.9681 - val_loss: 0.1413 - val_accuracy: 0.9637\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1254 - accuracy: 0.9688 - val_loss: 0.1497 - val_accuracy: 0.9590\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1322 - accuracy: 0.9656 - val_loss: 0.1734 - val_accuracy: 0.9498\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1584 - accuracy: 0.9545 - val_loss: 0.1567 - val_accuracy: 0.9567\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1360 - accuracy: 0.9644 - val_loss: 0.1600 - val_accuracy: 0.9553\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1354 - accuracy: 0.9642 - val_loss: 0.1434 - val_accuracy: 0.9623\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1282 - accuracy: 0.9670 - val_loss: 0.1428 - val_accuracy: 0.9625\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1251 - accuracy: 0.9693 - val_loss: 0.1443 - val_accuracy: 0.9618\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1332 - accuracy: 0.9654 - val_loss: 0.1594 - val_accuracy: 0.9557\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1492 - accuracy: 0.9584 - val_loss: 0.1555 - val_accuracy: 0.9572\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1472 - accuracy: 0.9595 - val_loss: 0.1984 - val_accuracy: 0.9392\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1521 - accuracy: 0.9573 - val_loss: 0.1425 - val_accuracy: 0.9635\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1321 - accuracy: 0.9663 - val_loss: 0.1460 - val_accuracy: 0.9612\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1290 - accuracy: 0.9676 - val_loss: 0.1492 - val_accuracy: 0.9599\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1297 - accuracy: 0.9666 - val_loss: 0.1471 - val_accuracy: 0.9604\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1260 - accuracy: 0.9688 - val_loss: 0.1407 - val_accuracy: 0.9635\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1255 - accuracy: 0.9687 - val_loss: 0.1446 - val_accuracy: 0.9616\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1265 - accuracy: 0.9686 - val_loss: 0.1409 - val_accuracy: 0.9632\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1250 - accuracy: 0.9689 - val_loss: 0.1437 - val_accuracy: 0.9613\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1267 - accuracy: 0.9675 - val_loss: 0.1392 - val_accuracy: 0.9634\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 1s 84ms/step - loss: 0.1212 - accuracy: 0.9708 - val_loss: 0.1420 - val_accuracy: 0.9629\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1216 - accuracy: 0.9704 - val_loss: 0.1550 - val_accuracy: 0.9573\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1529 - accuracy: 0.9571 - val_loss: 0.1450 - val_accuracy: 0.9625\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1452 - accuracy: 0.9610 - val_loss: 0.1505 - val_accuracy: 0.9604\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1282 - accuracy: 0.9679 - val_loss: 0.1378 - val_accuracy: 0.9648\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1222 - accuracy: 0.9700 - val_loss: 0.1399 - val_accuracy: 0.9638\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1301 - accuracy: 0.9664 - val_loss: 0.1445 - val_accuracy: 0.9610\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1259 - accuracy: 0.9683 - val_loss: 0.1422 - val_accuracy: 0.9620\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1233 - accuracy: 0.9698 - val_loss: 0.1370 - val_accuracy: 0.9652\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1214 - accuracy: 0.9703 - val_loss: 0.1495 - val_accuracy: 0.9588\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1350 - accuracy: 0.9636 - val_loss: 0.1381 - val_accuracy: 0.9638\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1266 - accuracy: 0.9679 - val_loss: 0.1500 - val_accuracy: 0.9586\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1263 - accuracy: 0.9682 - val_loss: 0.1463 - val_accuracy: 0.9595\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1321 - accuracy: 0.9652 - val_loss: 0.1560 - val_accuracy: 0.9562\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1401 - accuracy: 0.9623 - val_loss: 0.1485 - val_accuracy: 0.9605\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1442 - accuracy: 0.9610 - val_loss: 0.1429 - val_accuracy: 0.9632\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1272 - accuracy: 0.9681 - val_loss: 0.1426 - val_accuracy: 0.9626\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1223 - accuracy: 0.9697 - val_loss: 0.1402 - val_accuracy: 0.9635\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1221 - accuracy: 0.9696 - val_loss: 0.1373 - val_accuracy: 0.9645\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1214 - accuracy: 0.9704 - val_loss: 0.1344 - val_accuracy: 0.9659\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1182 - accuracy: 0.9719 - val_loss: 0.1776 - val_accuracy: 0.9475\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1405 - accuracy: 0.9618 - val_loss: 0.1397 - val_accuracy: 0.9634\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1214 - accuracy: 0.9703 - val_loss: 0.1357 - val_accuracy: 0.9650\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1202 - accuracy: 0.9706 - val_loss: 0.1342 - val_accuracy: 0.9653\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1164 - accuracy: 0.9722 - val_loss: 0.1481 - val_accuracy: 0.9593\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1266 - accuracy: 0.9677 - val_loss: 0.1355 - val_accuracy: 0.9658\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1206 - accuracy: 0.9710 - val_loss: 0.1348 - val_accuracy: 0.9651\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1205 - accuracy: 0.9704 - val_loss: 0.1967 - val_accuracy: 0.9414\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1635 - accuracy: 0.9529 - val_loss: 0.1653 - val_accuracy: 0.9536\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1326 - accuracy: 0.9650 - val_loss: 0.1366 - val_accuracy: 0.9647\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1244 - accuracy: 0.9686 - val_loss: 0.1350 - val_accuracy: 0.9649\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1170 - accuracy: 0.9715 - val_loss: 0.1411 - val_accuracy: 0.9631\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1195 - accuracy: 0.9709 - val_loss: 0.1439 - val_accuracy: 0.9615\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1239 - accuracy: 0.9686 - val_loss: 0.1572 - val_accuracy: 0.9558\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1280 - accuracy: 0.9674 - val_loss: 0.1370 - val_accuracy: 0.9645\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1181 - accuracy: 0.9713 - val_loss: 0.1360 - val_accuracy: 0.9641\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1153 - accuracy: 0.9725 - val_loss: 0.1311 - val_accuracy: 0.9669\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1161 - accuracy: 0.9723 - val_loss: 0.1313 - val_accuracy: 0.9667\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1187 - accuracy: 0.9706 - val_loss: 0.1319 - val_accuracy: 0.9667\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1149 - accuracy: 0.9728 - val_loss: 0.1299 - val_accuracy: 0.9676\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1128 - accuracy: 0.9739 - val_loss: 0.1355 - val_accuracy: 0.9650\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1184 - accuracy: 0.9709 - val_loss: 0.1361 - val_accuracy: 0.9646\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1173 - accuracy: 0.9717 - val_loss: 0.1698 - val_accuracy: 0.9506\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1343 - accuracy: 0.9645 - val_loss: 0.1377 - val_accuracy: 0.9644\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1219 - accuracy: 0.9695 - val_loss: 0.1367 - val_accuracy: 0.9637\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1181 - accuracy: 0.9705 - val_loss: 0.1318 - val_accuracy: 0.9667\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1156 - accuracy: 0.9724 - val_loss: 0.1490 - val_accuracy: 0.9594\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1251 - accuracy: 0.9682 - val_loss: 0.1504 - val_accuracy: 0.9593\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1234 - accuracy: 0.9691 - val_loss: 0.1362 - val_accuracy: 0.9652\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1196 - accuracy: 0.9707 - val_loss: 0.1329 - val_accuracy: 0.9661\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1143 - accuracy: 0.9732 - val_loss: 0.1267 - val_accuracy: 0.9684\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1112 - accuracy: 0.9740 - val_loss: 0.1289 - val_accuracy: 0.9672\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1217 - accuracy: 0.9693 - val_loss: 0.1363 - val_accuracy: 0.9653\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1180 - accuracy: 0.9709 - val_loss: 0.1356 - val_accuracy: 0.9648\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1236 - accuracy: 0.9687 - val_loss: 0.1620 - val_accuracy: 0.9543\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1359 - accuracy: 0.9633 - val_loss: 0.1497 - val_accuracy: 0.9580\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1349 - accuracy: 0.9639 - val_loss: 0.1407 - val_accuracy: 0.9621\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1216 - accuracy: 0.9693 - val_loss: 0.1492 - val_accuracy: 0.9595\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1202 - accuracy: 0.9696 - val_loss: 0.1403 - val_accuracy: 0.9630\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1163 - accuracy: 0.9717 - val_loss: 0.1281 - val_accuracy: 0.9679\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1131 - accuracy: 0.9739 - val_loss: 0.1374 - val_accuracy: 0.9635\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1228 - accuracy: 0.9686 - val_loss: 0.1715 - val_accuracy: 0.9511\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1411 - accuracy: 0.9619 - val_loss: 0.1529 - val_accuracy: 0.9585\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1282 - accuracy: 0.9670 - val_loss: 0.1268 - val_accuracy: 0.9685\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1123 - accuracy: 0.9737 - val_loss: 0.1260 - val_accuracy: 0.9689\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1107 - accuracy: 0.9741 - val_loss: 0.1276 - val_accuracy: 0.9681\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1103 - accuracy: 0.9746 - val_loss: 0.1307 - val_accuracy: 0.9666\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1136 - accuracy: 0.9723 - val_loss: 0.1319 - val_accuracy: 0.9660\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1203 - accuracy: 0.9696 - val_loss: 0.1258 - val_accuracy: 0.9689\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1119 - accuracy: 0.9738 - val_loss: 0.1260 - val_accuracy: 0.9684\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.1122 - accuracy: 0.9737 - val_loss: 0.1254 - val_accuracy: 0.9693\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1254 - accuracy: 0.9681 - val_loss: 0.1524 - val_accuracy: 0.9573\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1234 - accuracy: 0.9682 - val_loss: 0.1342 - val_accuracy: 0.9659\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1191 - accuracy: 0.9704 - val_loss: 0.1294 - val_accuracy: 0.9673\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1158 - accuracy: 0.9716 - val_loss: 0.1304 - val_accuracy: 0.9667\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1094 - accuracy: 0.9748 - val_loss: 0.1233 - val_accuracy: 0.9700\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1086 - accuracy: 0.9748 - val_loss: 0.1411 - val_accuracy: 0.9621\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1162 - accuracy: 0.9715 - val_loss: 0.1337 - val_accuracy: 0.9651\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1167 - accuracy: 0.9706 - val_loss: 0.1286 - val_accuracy: 0.9677\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1120 - accuracy: 0.9736 - val_loss: 0.1298 - val_accuracy: 0.9668\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1102 - accuracy: 0.9739 - val_loss: 0.1289 - val_accuracy: 0.9675\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1098 - accuracy: 0.9742 - val_loss: 0.1295 - val_accuracy: 0.9666\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1130 - accuracy: 0.9731 - val_loss: 0.1247 - val_accuracy: 0.9693\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1133 - accuracy: 0.9729 - val_loss: 0.1280 - val_accuracy: 0.9678\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1228 - accuracy: 0.9686 - val_loss: 0.1265 - val_accuracy: 0.9683\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1189 - accuracy: 0.9705 - val_loss: 0.1545 - val_accuracy: 0.9565\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1291 - accuracy: 0.9659 - val_loss: 0.1570 - val_accuracy: 0.9571\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1255 - accuracy: 0.9682 - val_loss: 0.1319 - val_accuracy: 0.9661\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1164 - accuracy: 0.9715 - val_loss: 0.1285 - val_accuracy: 0.9677\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1112 - accuracy: 0.9736 - val_loss: 0.1261 - val_accuracy: 0.9686\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1073 - accuracy: 0.9749 - val_loss: 0.1224 - val_accuracy: 0.9702\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1055 - accuracy: 0.9758 - val_loss: 0.1236 - val_accuracy: 0.9694\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1078 - accuracy: 0.9750 - val_loss: 0.1230 - val_accuracy: 0.9698\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1073 - accuracy: 0.9754 - val_loss: 0.1292 - val_accuracy: 0.9667\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1075 - accuracy: 0.9756 - val_loss: 0.1205 - val_accuracy: 0.9712\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1052 - accuracy: 0.9765 - val_loss: 0.1199 - val_accuracy: 0.9709\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1030 - accuracy: 0.9768 - val_loss: 0.1471 - val_accuracy: 0.9593\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1189 - accuracy: 0.9700 - val_loss: 0.1419 - val_accuracy: 0.9627\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1159 - accuracy: 0.9714 - val_loss: 0.1573 - val_accuracy: 0.9557\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1326 - accuracy: 0.9636 - val_loss: 0.1394 - val_accuracy: 0.9631\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1146 - accuracy: 0.9724 - val_loss: 0.1205 - val_accuracy: 0.9712\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1055 - accuracy: 0.9761 - val_loss: 0.1268 - val_accuracy: 0.9679\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1119 - accuracy: 0.9732 - val_loss: 0.1225 - val_accuracy: 0.9691\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1049 - accuracy: 0.9765 - val_loss: 0.1203 - val_accuracy: 0.9710\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1044 - accuracy: 0.9759 - val_loss: 0.1222 - val_accuracy: 0.9697\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1101 - accuracy: 0.9737 - val_loss: 0.1280 - val_accuracy: 0.9684\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1089 - accuracy: 0.9744 - val_loss: 0.1217 - val_accuracy: 0.9708\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1065 - accuracy: 0.9755 - val_loss: 0.1222 - val_accuracy: 0.9710\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1040 - accuracy: 0.9763 - val_loss: 0.1235 - val_accuracy: 0.9686\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1075 - accuracy: 0.9750 - val_loss: 0.1243 - val_accuracy: 0.9694\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1036 - accuracy: 0.9769 - val_loss: 0.1196 - val_accuracy: 0.9714\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1029 - accuracy: 0.9766 - val_loss: 0.1168 - val_accuracy: 0.9722\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1009 - accuracy: 0.9777 - val_loss: 0.1174 - val_accuracy: 0.9717\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1037 - accuracy: 0.9765 - val_loss: 0.1177 - val_accuracy: 0.9719\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1033 - accuracy: 0.9762 - val_loss: 0.1339 - val_accuracy: 0.9646\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1123 - accuracy: 0.9726 - val_loss: 0.1233 - val_accuracy: 0.9694\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1274 - accuracy: 0.9669 - val_loss: 0.1742 - val_accuracy: 0.9511\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1548 - accuracy: 0.9554 - val_loss: 0.1506 - val_accuracy: 0.9598\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1268 - accuracy: 0.9674 - val_loss: 0.1274 - val_accuracy: 0.9696\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1121 - accuracy: 0.9737 - val_loss: 0.1248 - val_accuracy: 0.9694\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1065 - accuracy: 0.9753 - val_loss: 0.1301 - val_accuracy: 0.9665\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1074 - accuracy: 0.9749 - val_loss: 0.1221 - val_accuracy: 0.9698\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1037 - accuracy: 0.9766 - val_loss: 0.1179 - val_accuracy: 0.9723\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1017 - accuracy: 0.9774 - val_loss: 0.1228 - val_accuracy: 0.9696\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1034 - accuracy: 0.9770 - val_loss: 0.1160 - val_accuracy: 0.9725\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0997 - accuracy: 0.9778 - val_loss: 0.1173 - val_accuracy: 0.9718\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0986 - accuracy: 0.9782 - val_loss: 0.1148 - val_accuracy: 0.9730\n",
            "Epoch 552/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1006 - accuracy: 0.9783 - val_loss: 0.1178 - val_accuracy: 0.9715\n",
            "Epoch 553/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0982 - accuracy: 0.9783 - val_loss: 0.1202 - val_accuracy: 0.9706\n",
            "Epoch 554/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1004 - accuracy: 0.9774 - val_loss: 0.1168 - val_accuracy: 0.9718\n",
            "Epoch 555/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1050 - accuracy: 0.9754 - val_loss: 0.1263 - val_accuracy: 0.9676\n",
            "Epoch 556/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1060 - accuracy: 0.9746 - val_loss: 0.1206 - val_accuracy: 0.9710\n",
            "Epoch 557/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1008 - accuracy: 0.9775 - val_loss: 0.1178 - val_accuracy: 0.9718\n",
            "Epoch 558/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0989 - accuracy: 0.9782 - val_loss: 0.1155 - val_accuracy: 0.9723\n",
            "Epoch 559/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0979 - accuracy: 0.9784 - val_loss: 0.1143 - val_accuracy: 0.9731\n",
            "Epoch 560/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1014 - accuracy: 0.9770 - val_loss: 0.1207 - val_accuracy: 0.9703\n",
            "Epoch 561/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1018 - accuracy: 0.9775 - val_loss: 0.1157 - val_accuracy: 0.9728\n",
            "Epoch 562/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0968 - accuracy: 0.9793 - val_loss: 0.1167 - val_accuracy: 0.9719\n",
            "Epoch 563/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0982 - accuracy: 0.9784 - val_loss: 0.1200 - val_accuracy: 0.9706\n",
            "Epoch 564/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1086 - accuracy: 0.9738 - val_loss: 0.1222 - val_accuracy: 0.9698\n",
            "Epoch 565/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1042 - accuracy: 0.9762 - val_loss: 0.1348 - val_accuracy: 0.9643\n",
            "Epoch 566/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1259 - accuracy: 0.9667 - val_loss: 0.1274 - val_accuracy: 0.9676\n",
            "Epoch 567/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1090 - accuracy: 0.9736 - val_loss: 0.1233 - val_accuracy: 0.9698\n",
            "Epoch 568/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1054 - accuracy: 0.9758 - val_loss: 0.1217 - val_accuracy: 0.9696\n",
            "Epoch 569/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1072 - accuracy: 0.9747 - val_loss: 0.1253 - val_accuracy: 0.9688\n",
            "Epoch 570/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1009 - accuracy: 0.9776 - val_loss: 0.1184 - val_accuracy: 0.9714\n",
            "Epoch 571/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0974 - accuracy: 0.9787 - val_loss: 0.1176 - val_accuracy: 0.9712\n",
            "Epoch 572/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1096 - accuracy: 0.9733 - val_loss: 0.1309 - val_accuracy: 0.9657\n",
            "Epoch 573/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1459 - accuracy: 0.9588 - val_loss: 0.1520 - val_accuracy: 0.9581\n",
            "Epoch 574/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1270 - accuracy: 0.9668 - val_loss: 0.1192 - val_accuracy: 0.9712\n",
            "Epoch 575/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1033 - accuracy: 0.9762 - val_loss: 0.1194 - val_accuracy: 0.9711\n",
            "Epoch 576/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1000 - accuracy: 0.9776 - val_loss: 0.1279 - val_accuracy: 0.9673\n",
            "Epoch 577/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1053 - accuracy: 0.9753 - val_loss: 0.1196 - val_accuracy: 0.9707\n",
            "Epoch 578/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1015 - accuracy: 0.9774 - val_loss: 0.1181 - val_accuracy: 0.9720\n",
            "Epoch 579/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0987 - accuracy: 0.9782 - val_loss: 0.1104 - val_accuracy: 0.9746\n",
            "Epoch 580/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0972 - accuracy: 0.9784 - val_loss: 0.1167 - val_accuracy: 0.9714\n",
            "Epoch 581/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1001 - accuracy: 0.9774 - val_loss: 0.1168 - val_accuracy: 0.9723\n",
            "Epoch 582/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0976 - accuracy: 0.9788 - val_loss: 0.1146 - val_accuracy: 0.9735\n",
            "Epoch 583/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0965 - accuracy: 0.9792 - val_loss: 0.1172 - val_accuracy: 0.9728\n",
            "Epoch 584/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1010 - accuracy: 0.9771 - val_loss: 0.1165 - val_accuracy: 0.9717\n",
            "Epoch 585/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0977 - accuracy: 0.9785 - val_loss: 0.1298 - val_accuracy: 0.9668\n",
            "Epoch 586/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1105 - accuracy: 0.9733 - val_loss: 0.1107 - val_accuracy: 0.9744\n",
            "Epoch 587/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0972 - accuracy: 0.9792 - val_loss: 0.1105 - val_accuracy: 0.9750\n",
            "Epoch 588/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0988 - accuracy: 0.9782 - val_loss: 0.2673 - val_accuracy: 0.9218\n",
            "Epoch 589/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2515 - accuracy: 0.9251 - val_loss: 0.1577 - val_accuracy: 0.9561\n",
            "Epoch 590/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1398 - accuracy: 0.9633 - val_loss: 0.1317 - val_accuracy: 0.9658\n",
            "Epoch 591/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1121 - accuracy: 0.9732 - val_loss: 0.1191 - val_accuracy: 0.9712\n",
            "Epoch 592/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1001 - accuracy: 0.9774 - val_loss: 0.1133 - val_accuracy: 0.9740\n",
            "Epoch 593/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0965 - accuracy: 0.9791 - val_loss: 0.1195 - val_accuracy: 0.9714\n",
            "Epoch 594/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0995 - accuracy: 0.9782 - val_loss: 0.1097 - val_accuracy: 0.9754\n",
            "Epoch 595/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0939 - accuracy: 0.9801 - val_loss: 0.1109 - val_accuracy: 0.9743\n",
            "Epoch 596/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0928 - accuracy: 0.9805 - val_loss: 0.1080 - val_accuracy: 0.9758\n",
            "Epoch 597/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0941 - accuracy: 0.9801 - val_loss: 0.1110 - val_accuracy: 0.9742\n",
            "Epoch 598/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0940 - accuracy: 0.9799 - val_loss: 0.1119 - val_accuracy: 0.9741\n",
            "Epoch 599/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0975 - accuracy: 0.9781 - val_loss: 0.1135 - val_accuracy: 0.9733\n",
            "Epoch 600/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0943 - accuracy: 0.9797 - val_loss: 0.1118 - val_accuracy: 0.9741\n",
            "Epoch 601/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0929 - accuracy: 0.9803 - val_loss: 0.1102 - val_accuracy: 0.9742\n",
            "Epoch 602/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0937 - accuracy: 0.9802 - val_loss: 0.1125 - val_accuracy: 0.9733\n",
            "Epoch 603/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0920 - accuracy: 0.9805 - val_loss: 0.1079 - val_accuracy: 0.9762\n",
            "Epoch 604/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0935 - accuracy: 0.9800 - val_loss: 0.1175 - val_accuracy: 0.9717\n",
            "Epoch 605/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0969 - accuracy: 0.9785 - val_loss: 0.1375 - val_accuracy: 0.9641\n",
            "Epoch 606/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1822 - accuracy: 0.9472 - val_loss: 0.1797 - val_accuracy: 0.9463\n",
            "Epoch 607/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1473 - accuracy: 0.9586 - val_loss: 0.1530 - val_accuracy: 0.9567\n",
            "Epoch 608/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1207 - accuracy: 0.9688 - val_loss: 0.1284 - val_accuracy: 0.9669\n",
            "Epoch 609/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1120 - accuracy: 0.9729 - val_loss: 0.1244 - val_accuracy: 0.9687\n",
            "Epoch 610/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1044 - accuracy: 0.9758 - val_loss: 0.1135 - val_accuracy: 0.9738\n",
            "Epoch 611/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0958 - accuracy: 0.9795 - val_loss: 0.1113 - val_accuracy: 0.9752\n",
            "Epoch 612/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0948 - accuracy: 0.9796 - val_loss: 0.1089 - val_accuracy: 0.9757\n",
            "Epoch 613/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0941 - accuracy: 0.9803 - val_loss: 0.1090 - val_accuracy: 0.9751\n",
            "Epoch 614/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0923 - accuracy: 0.9805 - val_loss: 0.1082 - val_accuracy: 0.9757\n",
            "Epoch 615/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0923 - accuracy: 0.9803 - val_loss: 0.1094 - val_accuracy: 0.9745\n",
            "Epoch 616/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0931 - accuracy: 0.9800 - val_loss: 0.1190 - val_accuracy: 0.9716\n",
            "Epoch 617/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0958 - accuracy: 0.9789 - val_loss: 0.1060 - val_accuracy: 0.9765\n",
            "Epoch 618/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0910 - accuracy: 0.9812 - val_loss: 0.1056 - val_accuracy: 0.9768\n",
            "Epoch 619/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0888 - accuracy: 0.9817 - val_loss: 0.1106 - val_accuracy: 0.9744\n",
            "Epoch 620/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0926 - accuracy: 0.9804 - val_loss: 0.1180 - val_accuracy: 0.9715\n",
            "Epoch 621/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0952 - accuracy: 0.9795 - val_loss: 0.1075 - val_accuracy: 0.9759\n",
            "Epoch 622/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0925 - accuracy: 0.9805 - val_loss: 0.1145 - val_accuracy: 0.9725\n",
            "Epoch 623/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0930 - accuracy: 0.9803 - val_loss: 0.1094 - val_accuracy: 0.9748\n",
            "Epoch 624/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0925 - accuracy: 0.9805 - val_loss: 0.1074 - val_accuracy: 0.9759\n",
            "Epoch 625/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0955 - accuracy: 0.9788 - val_loss: 0.1071 - val_accuracy: 0.9762\n",
            "Epoch 626/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0922 - accuracy: 0.9804 - val_loss: 0.1119 - val_accuracy: 0.9740\n",
            "Epoch 627/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0945 - accuracy: 0.9793 - val_loss: 0.1056 - val_accuracy: 0.9769\n",
            "Epoch 628/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0947 - accuracy: 0.9795 - val_loss: 0.1356 - val_accuracy: 0.9631\n",
            "Epoch 629/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1054 - accuracy: 0.9750 - val_loss: 0.1270 - val_accuracy: 0.9681\n",
            "Epoch 630/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1050 - accuracy: 0.9754 - val_loss: 0.1147 - val_accuracy: 0.9715\n",
            "Epoch 631/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1019 - accuracy: 0.9767 - val_loss: 0.1114 - val_accuracy: 0.9738\n",
            "Epoch 632/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0947 - accuracy: 0.9795 - val_loss: 0.1060 - val_accuracy: 0.9765\n",
            "Epoch 633/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0904 - accuracy: 0.9809 - val_loss: 0.1053 - val_accuracy: 0.9768\n",
            "Epoch 634/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0884 - accuracy: 0.9817 - val_loss: 0.1079 - val_accuracy: 0.9750\n",
            "Epoch 635/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0902 - accuracy: 0.9811 - val_loss: 0.1073 - val_accuracy: 0.9761\n",
            "Epoch 636/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0887 - accuracy: 0.9815 - val_loss: 0.1152 - val_accuracy: 0.9717\n",
            "Epoch 637/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0965 - accuracy: 0.9783 - val_loss: 0.1105 - val_accuracy: 0.9749\n",
            "Epoch 638/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0932 - accuracy: 0.9801 - val_loss: 0.1120 - val_accuracy: 0.9739\n",
            "Epoch 639/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0934 - accuracy: 0.9796 - val_loss: 0.1080 - val_accuracy: 0.9758\n",
            "Epoch 640/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0966 - accuracy: 0.9787 - val_loss: 0.1132 - val_accuracy: 0.9730\n",
            "Epoch 641/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0976 - accuracy: 0.9785 - val_loss: 0.1142 - val_accuracy: 0.9731\n",
            "Epoch 642/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0966 - accuracy: 0.9781 - val_loss: 0.1718 - val_accuracy: 0.9536\n",
            "Epoch 643/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1339 - accuracy: 0.9644 - val_loss: 0.1187 - val_accuracy: 0.9712\n",
            "Epoch 644/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1097 - accuracy: 0.9735 - val_loss: 0.1170 - val_accuracy: 0.9712\n",
            "Epoch 645/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0978 - accuracy: 0.9779 - val_loss: 0.1100 - val_accuracy: 0.9750\n",
            "Epoch 646/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0921 - accuracy: 0.9808 - val_loss: 0.1067 - val_accuracy: 0.9761\n",
            "Epoch 647/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0897 - accuracy: 0.9815 - val_loss: 0.1055 - val_accuracy: 0.9769\n",
            "Epoch 648/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0896 - accuracy: 0.9814 - val_loss: 0.1064 - val_accuracy: 0.9765\n",
            "Epoch 649/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0926 - accuracy: 0.9799 - val_loss: 0.1053 - val_accuracy: 0.9766\n",
            "Epoch 650/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0944 - accuracy: 0.9789 - val_loss: 0.1100 - val_accuracy: 0.9739\n",
            "Epoch 651/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0909 - accuracy: 0.9810 - val_loss: 0.1130 - val_accuracy: 0.9727\n",
            "Epoch 652/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0923 - accuracy: 0.9804 - val_loss: 0.1056 - val_accuracy: 0.9767\n",
            "Epoch 653/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0868 - accuracy: 0.9823 - val_loss: 0.1039 - val_accuracy: 0.9768\n",
            "Epoch 654/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0872 - accuracy: 0.9822 - val_loss: 0.1073 - val_accuracy: 0.9754\n",
            "Epoch 655/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0901 - accuracy: 0.9811 - val_loss: 0.1036 - val_accuracy: 0.9771\n",
            "Epoch 656/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0887 - accuracy: 0.9818 - val_loss: 0.1078 - val_accuracy: 0.9748\n",
            "Epoch 657/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0876 - accuracy: 0.9819 - val_loss: 0.1024 - val_accuracy: 0.9778\n",
            "Epoch 658/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0858 - accuracy: 0.9828 - val_loss: 0.1029 - val_accuracy: 0.9770\n",
            "Epoch 659/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0855 - accuracy: 0.9827 - val_loss: 0.1064 - val_accuracy: 0.9758\n",
            "Epoch 660/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0868 - accuracy: 0.9826 - val_loss: 0.1086 - val_accuracy: 0.9760\n",
            "Epoch 661/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0883 - accuracy: 0.9818 - val_loss: 0.1290 - val_accuracy: 0.9652\n",
            "Epoch 662/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1128 - accuracy: 0.9713 - val_loss: 0.1491 - val_accuracy: 0.9604\n",
            "Epoch 663/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1271 - accuracy: 0.9658 - val_loss: 0.1261 - val_accuracy: 0.9686\n",
            "Epoch 664/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1035 - accuracy: 0.9759 - val_loss: 0.1068 - val_accuracy: 0.9758\n",
            "Epoch 665/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0892 - accuracy: 0.9822 - val_loss: 0.1062 - val_accuracy: 0.9756\n",
            "Epoch 666/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0857 - accuracy: 0.9823 - val_loss: 0.1021 - val_accuracy: 0.9777\n",
            "Epoch 667/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0843 - accuracy: 0.9836 - val_loss: 0.1060 - val_accuracy: 0.9758\n",
            "Epoch 668/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1013 - accuracy: 0.9761 - val_loss: 0.1206 - val_accuracy: 0.9705\n",
            "Epoch 669/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0978 - accuracy: 0.9781 - val_loss: 0.1059 - val_accuracy: 0.9759\n",
            "Epoch 670/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0928 - accuracy: 0.9799 - val_loss: 0.1112 - val_accuracy: 0.9736\n",
            "Epoch 671/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0894 - accuracy: 0.9815 - val_loss: 0.1036 - val_accuracy: 0.9771\n",
            "Epoch 672/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0882 - accuracy: 0.9818 - val_loss: 0.1159 - val_accuracy: 0.9708\n",
            "Epoch 673/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0944 - accuracy: 0.9791 - val_loss: 0.1032 - val_accuracy: 0.9780\n",
            "Epoch 674/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0975 - accuracy: 0.9781 - val_loss: 0.1402 - val_accuracy: 0.9635\n",
            "Epoch 675/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1224 - accuracy: 0.9688 - val_loss: 0.1168 - val_accuracy: 0.9717\n",
            "Epoch 676/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0961 - accuracy: 0.9792 - val_loss: 0.1076 - val_accuracy: 0.9756\n",
            "Epoch 677/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0905 - accuracy: 0.9807 - val_loss: 0.1055 - val_accuracy: 0.9758\n",
            "Epoch 678/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0948 - accuracy: 0.9787 - val_loss: 0.1023 - val_accuracy: 0.9775\n",
            "Epoch 679/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0868 - accuracy: 0.9824 - val_loss: 0.1042 - val_accuracy: 0.9767\n",
            "Epoch 680/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0858 - accuracy: 0.9822 - val_loss: 0.1009 - val_accuracy: 0.9786\n",
            "Epoch 681/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0858 - accuracy: 0.9827 - val_loss: 0.1003 - val_accuracy: 0.9785\n",
            "Epoch 682/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0840 - accuracy: 0.9836 - val_loss: 0.1033 - val_accuracy: 0.9771\n",
            "Epoch 683/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0845 - accuracy: 0.9830 - val_loss: 0.1070 - val_accuracy: 0.9752\n",
            "Epoch 684/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0893 - accuracy: 0.9811 - val_loss: 0.1118 - val_accuracy: 0.9732\n",
            "Epoch 685/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0923 - accuracy: 0.9796 - val_loss: 0.1057 - val_accuracy: 0.9755\n",
            "Epoch 686/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0892 - accuracy: 0.9812 - val_loss: 0.1008 - val_accuracy: 0.9782\n",
            "Epoch 687/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0868 - accuracy: 0.9821 - val_loss: 0.1088 - val_accuracy: 0.9742\n",
            "Epoch 688/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0913 - accuracy: 0.9799 - val_loss: 0.1030 - val_accuracy: 0.9767\n",
            "Epoch 689/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0857 - accuracy: 0.9827 - val_loss: 0.1000 - val_accuracy: 0.9782\n",
            "Epoch 690/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0852 - accuracy: 0.9830 - val_loss: 0.1033 - val_accuracy: 0.9769\n",
            "Epoch 691/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0853 - accuracy: 0.9826 - val_loss: 0.1122 - val_accuracy: 0.9731\n",
            "Epoch 692/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1408 - accuracy: 0.9641 - val_loss: 0.1813 - val_accuracy: 0.9443\n",
            "Epoch 693/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1394 - accuracy: 0.9610 - val_loss: 0.1209 - val_accuracy: 0.9701\n",
            "Epoch 694/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1002 - accuracy: 0.9778 - val_loss: 0.1094 - val_accuracy: 0.9740\n",
            "Epoch 695/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0907 - accuracy: 0.9808 - val_loss: 0.1037 - val_accuracy: 0.9768\n",
            "Epoch 696/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0848 - accuracy: 0.9829 - val_loss: 0.0999 - val_accuracy: 0.9782\n",
            "Epoch 697/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0847 - accuracy: 0.9830 - val_loss: 0.0982 - val_accuracy: 0.9795\n",
            "Epoch 698/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0815 - accuracy: 0.9843 - val_loss: 0.0971 - val_accuracy: 0.9788\n",
            "Epoch 699/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0803 - accuracy: 0.9847 - val_loss: 0.0986 - val_accuracy: 0.9782\n",
            "Epoch 700/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0825 - accuracy: 0.9838 - val_loss: 0.1027 - val_accuracy: 0.9767\n",
            "Epoch 701/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0853 - accuracy: 0.9826 - val_loss: 0.1039 - val_accuracy: 0.9758\n",
            "Epoch 702/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0833 - accuracy: 0.9831 - val_loss: 0.1000 - val_accuracy: 0.9779\n",
            "Epoch 703/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0823 - accuracy: 0.9838 - val_loss: 0.1025 - val_accuracy: 0.9776\n",
            "Epoch 704/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0856 - accuracy: 0.9823 - val_loss: 0.1042 - val_accuracy: 0.9764\n",
            "Epoch 705/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0864 - accuracy: 0.9820 - val_loss: 0.1087 - val_accuracy: 0.9745\n",
            "Epoch 706/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0929 - accuracy: 0.9793 - val_loss: 0.1060 - val_accuracy: 0.9768\n",
            "Epoch 707/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0911 - accuracy: 0.9805 - val_loss: 0.1410 - val_accuracy: 0.9628\n",
            "Epoch 708/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1139 - accuracy: 0.9709 - val_loss: 0.1217 - val_accuracy: 0.9686\n",
            "Epoch 709/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0985 - accuracy: 0.9775 - val_loss: 0.1047 - val_accuracy: 0.9758\n",
            "Epoch 710/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0885 - accuracy: 0.9810 - val_loss: 0.1031 - val_accuracy: 0.9775\n",
            "Epoch 711/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0857 - accuracy: 0.9826 - val_loss: 0.1015 - val_accuracy: 0.9770\n",
            "Epoch 712/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0827 - accuracy: 0.9839 - val_loss: 0.0976 - val_accuracy: 0.9792\n",
            "Epoch 713/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0811 - accuracy: 0.9846 - val_loss: 0.0969 - val_accuracy: 0.9793\n",
            "Epoch 714/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0805 - accuracy: 0.9847 - val_loss: 0.1010 - val_accuracy: 0.9779\n",
            "Epoch 715/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0807 - accuracy: 0.9844 - val_loss: 0.0997 - val_accuracy: 0.9779\n",
            "Epoch 716/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0803 - accuracy: 0.9848 - val_loss: 0.0960 - val_accuracy: 0.9794\n",
            "Epoch 717/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0803 - accuracy: 0.9848 - val_loss: 0.0981 - val_accuracy: 0.9791\n",
            "Epoch 718/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0803 - accuracy: 0.9849 - val_loss: 0.1089 - val_accuracy: 0.9737\n",
            "Epoch 719/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1054 - accuracy: 0.9748 - val_loss: 0.1847 - val_accuracy: 0.9471\n",
            "Epoch 720/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.1589 - accuracy: 0.9546 - val_loss: 0.1509 - val_accuracy: 0.9573\n",
            "Epoch 721/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1127 - accuracy: 0.9721 - val_loss: 0.1059 - val_accuracy: 0.9760\n",
            "Epoch 722/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0916 - accuracy: 0.9802 - val_loss: 0.1017 - val_accuracy: 0.9775\n",
            "Epoch 723/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0837 - accuracy: 0.9837 - val_loss: 0.0970 - val_accuracy: 0.9795\n",
            "Epoch 724/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0804 - accuracy: 0.9850 - val_loss: 0.0969 - val_accuracy: 0.9786\n",
            "Epoch 725/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0802 - accuracy: 0.9846 - val_loss: 0.0960 - val_accuracy: 0.9798\n",
            "Epoch 726/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0803 - accuracy: 0.9849 - val_loss: 0.0949 - val_accuracy: 0.9808\n",
            "Epoch 727/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0790 - accuracy: 0.9854 - val_loss: 0.0988 - val_accuracy: 0.9785\n",
            "Epoch 728/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0814 - accuracy: 0.9837 - val_loss: 0.1036 - val_accuracy: 0.9767\n",
            "Epoch 729/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0831 - accuracy: 0.9836 - val_loss: 0.0990 - val_accuracy: 0.9776\n",
            "Epoch 730/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0813 - accuracy: 0.9841 - val_loss: 0.0942 - val_accuracy: 0.9802\n",
            "Epoch 731/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0835 - accuracy: 0.9831 - val_loss: 0.1005 - val_accuracy: 0.9777\n",
            "Epoch 732/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0847 - accuracy: 0.9826 - val_loss: 0.1028 - val_accuracy: 0.9763\n",
            "Epoch 733/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0833 - accuracy: 0.9828 - val_loss: 0.0961 - val_accuracy: 0.9788\n",
            "Epoch 734/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0812 - accuracy: 0.9841 - val_loss: 0.0978 - val_accuracy: 0.9786\n",
            "Epoch 735/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0818 - accuracy: 0.9836 - val_loss: 0.1130 - val_accuracy: 0.9715\n",
            "Epoch 736/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0916 - accuracy: 0.9801 - val_loss: 0.0990 - val_accuracy: 0.9783\n",
            "Epoch 737/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0835 - accuracy: 0.9832 - val_loss: 0.0945 - val_accuracy: 0.9803\n",
            "Epoch 738/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0822 - accuracy: 0.9838 - val_loss: 0.1052 - val_accuracy: 0.9755\n",
            "Epoch 739/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0841 - accuracy: 0.9827 - val_loss: 0.0941 - val_accuracy: 0.9802\n",
            "Epoch 740/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0822 - accuracy: 0.9836 - val_loss: 0.0954 - val_accuracy: 0.9800\n",
            "Epoch 741/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0822 - accuracy: 0.9836 - val_loss: 0.1012 - val_accuracy: 0.9762\n",
            "Epoch 742/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0818 - accuracy: 0.9840 - val_loss: 0.0963 - val_accuracy: 0.9796\n",
            "Epoch 743/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0809 - accuracy: 0.9839 - val_loss: 0.0971 - val_accuracy: 0.9793\n",
            "Epoch 744/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0811 - accuracy: 0.9840 - val_loss: 0.0984 - val_accuracy: 0.9779\n",
            "Epoch 745/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0787 - accuracy: 0.9848 - val_loss: 0.0929 - val_accuracy: 0.9804\n",
            "Epoch 746/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0788 - accuracy: 0.9852 - val_loss: 0.0949 - val_accuracy: 0.9798\n",
            "Epoch 747/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0823 - accuracy: 0.9834 - val_loss: 0.1203 - val_accuracy: 0.9694\n",
            "Epoch 748/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0885 - accuracy: 0.9804 - val_loss: 0.0953 - val_accuracy: 0.9800\n",
            "Epoch 749/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0782 - accuracy: 0.9853 - val_loss: 0.1021 - val_accuracy: 0.9765\n",
            "Epoch 750/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0825 - accuracy: 0.9835 - val_loss: 0.0930 - val_accuracy: 0.9805\n",
            "Epoch 751/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0799 - accuracy: 0.9844 - val_loss: 0.1017 - val_accuracy: 0.9768\n",
            "Epoch 752/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0818 - accuracy: 0.9836 - val_loss: 0.1083 - val_accuracy: 0.9732\n",
            "Epoch 753/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0922 - accuracy: 0.9789 - val_loss: 0.0921 - val_accuracy: 0.9811\n",
            "Epoch 754/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0853 - accuracy: 0.9822 - val_loss: 0.0954 - val_accuracy: 0.9797\n",
            "Epoch 755/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0775 - accuracy: 0.9857 - val_loss: 0.0989 - val_accuracy: 0.9783\n",
            "Epoch 756/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0796 - accuracy: 0.9847 - val_loss: 0.0935 - val_accuracy: 0.9807\n",
            "Epoch 757/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0746 - accuracy: 0.9867 - val_loss: 0.0925 - val_accuracy: 0.9808\n",
            "Epoch 758/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0754 - accuracy: 0.9868 - val_loss: 0.0947 - val_accuracy: 0.9790\n",
            "Epoch 759/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1177 - accuracy: 0.9725 - val_loss: 0.2974 - val_accuracy: 0.9155\n",
            "Epoch 760/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1984 - accuracy: 0.9434 - val_loss: 0.1352 - val_accuracy: 0.9639\n",
            "Epoch 761/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1165 - accuracy: 0.9709 - val_loss: 0.1093 - val_accuracy: 0.9743\n",
            "Epoch 762/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0921 - accuracy: 0.9800 - val_loss: 0.1016 - val_accuracy: 0.9769\n",
            "Epoch 763/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0832 - accuracy: 0.9832 - val_loss: 0.0965 - val_accuracy: 0.9787\n",
            "Epoch 764/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0791 - accuracy: 0.9852 - val_loss: 0.0940 - val_accuracy: 0.9802\n",
            "Epoch 765/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0777 - accuracy: 0.9852 - val_loss: 0.0918 - val_accuracy: 0.9803\n",
            "Epoch 766/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0768 - accuracy: 0.9856 - val_loss: 0.0919 - val_accuracy: 0.9804\n",
            "Epoch 767/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0747 - accuracy: 0.9867 - val_loss: 0.0916 - val_accuracy: 0.9803\n",
            "Epoch 768/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0762 - accuracy: 0.9863 - val_loss: 0.0941 - val_accuracy: 0.9795\n",
            "Epoch 769/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0748 - accuracy: 0.9865 - val_loss: 0.0908 - val_accuracy: 0.9809\n",
            "Epoch 770/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0747 - accuracy: 0.9869 - val_loss: 0.0979 - val_accuracy: 0.9781\n",
            "Epoch 771/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0807 - accuracy: 0.9840 - val_loss: 0.0955 - val_accuracy: 0.9792\n",
            "Epoch 772/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0825 - accuracy: 0.9832 - val_loss: 0.1005 - val_accuracy: 0.9776\n",
            "Epoch 773/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0873 - accuracy: 0.9813 - val_loss: 0.1073 - val_accuracy: 0.9749\n",
            "Epoch 774/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0897 - accuracy: 0.9804 - val_loss: 0.0964 - val_accuracy: 0.9794\n",
            "Epoch 775/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0867 - accuracy: 0.9814 - val_loss: 0.0954 - val_accuracy: 0.9786\n",
            "Epoch 776/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0804 - accuracy: 0.9844 - val_loss: 0.0926 - val_accuracy: 0.9806\n",
            "Epoch 777/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0757 - accuracy: 0.9864 - val_loss: 0.0898 - val_accuracy: 0.9818\n",
            "Epoch 778/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0753 - accuracy: 0.9863 - val_loss: 0.1015 - val_accuracy: 0.9764\n",
            "Epoch 779/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0804 - accuracy: 0.9838 - val_loss: 0.0941 - val_accuracy: 0.9798\n",
            "Epoch 780/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0771 - accuracy: 0.9857 - val_loss: 0.0941 - val_accuracy: 0.9797\n",
            "Epoch 781/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0767 - accuracy: 0.9857 - val_loss: 0.0915 - val_accuracy: 0.9808\n",
            "Epoch 782/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0736 - accuracy: 0.9870 - val_loss: 0.0889 - val_accuracy: 0.9821\n",
            "Epoch 783/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0732 - accuracy: 0.9870 - val_loss: 0.0914 - val_accuracy: 0.9802\n",
            "Epoch 784/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0812 - accuracy: 0.9837 - val_loss: 0.1248 - val_accuracy: 0.9669\n",
            "Epoch 785/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1017 - accuracy: 0.9753 - val_loss: 0.1013 - val_accuracy: 0.9781\n",
            "Epoch 786/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1018 - accuracy: 0.9753 - val_loss: 0.1164 - val_accuracy: 0.9707\n",
            "Epoch 787/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0932 - accuracy: 0.9787 - val_loss: 0.1101 - val_accuracy: 0.9731\n",
            "Epoch 788/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0894 - accuracy: 0.9801 - val_loss: 0.1036 - val_accuracy: 0.9755\n",
            "Epoch 789/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0831 - accuracy: 0.9827 - val_loss: 0.0905 - val_accuracy: 0.9814\n",
            "Epoch 790/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0755 - accuracy: 0.9858 - val_loss: 0.0903 - val_accuracy: 0.9814\n",
            "Epoch 791/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0748 - accuracy: 0.9861 - val_loss: 0.0929 - val_accuracy: 0.9802\n",
            "Epoch 792/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0764 - accuracy: 0.9859 - val_loss: 0.0901 - val_accuracy: 0.9809\n",
            "Epoch 793/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0729 - accuracy: 0.9869 - val_loss: 0.0896 - val_accuracy: 0.9810\n",
            "Epoch 794/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0733 - accuracy: 0.9870 - val_loss: 0.0905 - val_accuracy: 0.9810\n",
            "Epoch 795/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0769 - accuracy: 0.9852 - val_loss: 0.0902 - val_accuracy: 0.9809\n",
            "Epoch 796/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0741 - accuracy: 0.9865 - val_loss: 0.0895 - val_accuracy: 0.9812\n",
            "Epoch 797/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0721 - accuracy: 0.9871 - val_loss: 0.0934 - val_accuracy: 0.9791\n",
            "Epoch 798/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0730 - accuracy: 0.9869 - val_loss: 0.0880 - val_accuracy: 0.9820\n",
            "Epoch 799/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0736 - accuracy: 0.9867 - val_loss: 0.0897 - val_accuracy: 0.9814\n",
            "Epoch 800/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0731 - accuracy: 0.9867 - val_loss: 0.0881 - val_accuracy: 0.9814\n",
            "Epoch 801/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0747 - accuracy: 0.9862 - val_loss: 0.0917 - val_accuracy: 0.9807\n",
            "Epoch 802/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0745 - accuracy: 0.9863 - val_loss: 0.0900 - val_accuracy: 0.9806\n",
            "Epoch 803/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0753 - accuracy: 0.9861 - val_loss: 0.0910 - val_accuracy: 0.9803\n",
            "Epoch 804/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0730 - accuracy: 0.9869 - val_loss: 0.0879 - val_accuracy: 0.9818\n",
            "Epoch 805/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0734 - accuracy: 0.9866 - val_loss: 0.0915 - val_accuracy: 0.9807\n",
            "Epoch 806/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0739 - accuracy: 0.9864 - val_loss: 0.0871 - val_accuracy: 0.9820\n",
            "Epoch 807/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0708 - accuracy: 0.9878 - val_loss: 0.0888 - val_accuracy: 0.9816\n",
            "Epoch 808/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0719 - accuracy: 0.9872 - val_loss: 0.0899 - val_accuracy: 0.9809\n",
            "Epoch 809/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0747 - accuracy: 0.9861 - val_loss: 0.0904 - val_accuracy: 0.9804\n",
            "Epoch 810/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0763 - accuracy: 0.9854 - val_loss: 0.0938 - val_accuracy: 0.9783\n",
            "Epoch 811/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0782 - accuracy: 0.9846 - val_loss: 0.0977 - val_accuracy: 0.9777\n",
            "Epoch 812/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0934 - accuracy: 0.9783 - val_loss: 0.1130 - val_accuracy: 0.9715\n",
            "Epoch 813/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1150 - accuracy: 0.9705 - val_loss: 0.1120 - val_accuracy: 0.9720\n",
            "Epoch 814/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1047 - accuracy: 0.9739 - val_loss: 0.1229 - val_accuracy: 0.9681\n",
            "Epoch 815/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1014 - accuracy: 0.9753 - val_loss: 0.1001 - val_accuracy: 0.9773\n",
            "Epoch 816/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0836 - accuracy: 0.9830 - val_loss: 0.0983 - val_accuracy: 0.9780\n",
            "Epoch 817/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0787 - accuracy: 0.9848 - val_loss: 0.0897 - val_accuracy: 0.9809\n",
            "Epoch 818/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0737 - accuracy: 0.9866 - val_loss: 0.0883 - val_accuracy: 0.9815\n",
            "Epoch 819/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0718 - accuracy: 0.9873 - val_loss: 0.0883 - val_accuracy: 0.9815\n",
            "Epoch 820/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0716 - accuracy: 0.9871 - val_loss: 0.0858 - val_accuracy: 0.9831\n",
            "Epoch 821/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0701 - accuracy: 0.9881 - val_loss: 0.0871 - val_accuracy: 0.9820\n",
            "Epoch 822/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0701 - accuracy: 0.9877 - val_loss: 0.0846 - val_accuracy: 0.9834\n",
            "Epoch 823/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0696 - accuracy: 0.9884 - val_loss: 0.0871 - val_accuracy: 0.9823\n",
            "Epoch 824/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0726 - accuracy: 0.9868 - val_loss: 0.0863 - val_accuracy: 0.9826\n",
            "Epoch 825/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0707 - accuracy: 0.9874 - val_loss: 0.0860 - val_accuracy: 0.9828\n",
            "Epoch 826/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0697 - accuracy: 0.9880 - val_loss: 0.0852 - val_accuracy: 0.9828\n",
            "Epoch 827/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0700 - accuracy: 0.9880 - val_loss: 0.0838 - val_accuracy: 0.9832\n",
            "Epoch 828/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0683 - accuracy: 0.9885 - val_loss: 0.0854 - val_accuracy: 0.9830\n",
            "Epoch 829/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0691 - accuracy: 0.9882 - val_loss: 0.0840 - val_accuracy: 0.9836\n",
            "Epoch 830/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0718 - accuracy: 0.9873 - val_loss: 0.0997 - val_accuracy: 0.9765\n",
            "Epoch 831/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0792 - accuracy: 0.9838 - val_loss: 0.1036 - val_accuracy: 0.9758\n",
            "Epoch 832/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0849 - accuracy: 0.9815 - val_loss: 0.0966 - val_accuracy: 0.9778\n",
            "Epoch 833/1000\n",
            "11/11 [==============================] - 1s 85ms/step - loss: 0.0769 - accuracy: 0.9852 - val_loss: 0.0888 - val_accuracy: 0.9814\n",
            "Epoch 834/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0729 - accuracy: 0.9870 - val_loss: 0.0875 - val_accuracy: 0.9821\n",
            "Epoch 835/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0723 - accuracy: 0.9869 - val_loss: 0.0942 - val_accuracy: 0.9796\n",
            "Epoch 836/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0807 - accuracy: 0.9831 - val_loss: 0.0918 - val_accuracy: 0.9800\n",
            "Epoch 837/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1106 - accuracy: 0.9724 - val_loss: 0.1601 - val_accuracy: 0.9545\n",
            "Epoch 838/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1301 - accuracy: 0.9647 - val_loss: 0.0996 - val_accuracy: 0.9772\n",
            "Epoch 839/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0861 - accuracy: 0.9820 - val_loss: 0.0932 - val_accuracy: 0.9793\n",
            "Epoch 840/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0760 - accuracy: 0.9858 - val_loss: 0.0908 - val_accuracy: 0.9806\n",
            "Epoch 841/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0732 - accuracy: 0.9864 - val_loss: 0.0887 - val_accuracy: 0.9815\n",
            "Epoch 842/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0702 - accuracy: 0.9882 - val_loss: 0.0844 - val_accuracy: 0.9831\n",
            "Epoch 843/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0692 - accuracy: 0.9881 - val_loss: 0.0876 - val_accuracy: 0.9811\n",
            "Epoch 844/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0742 - accuracy: 0.9860 - val_loss: 0.0860 - val_accuracy: 0.9826\n",
            "Epoch 845/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0692 - accuracy: 0.9884 - val_loss: 0.0840 - val_accuracy: 0.9832\n",
            "Epoch 846/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0696 - accuracy: 0.9881 - val_loss: 0.0842 - val_accuracy: 0.9829\n",
            "Epoch 847/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0690 - accuracy: 0.9887 - val_loss: 0.0867 - val_accuracy: 0.9818\n",
            "Epoch 848/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0718 - accuracy: 0.9870 - val_loss: 0.0845 - val_accuracy: 0.9828\n",
            "Epoch 849/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0730 - accuracy: 0.9866 - val_loss: 0.1557 - val_accuracy: 0.9547\n",
            "Epoch 850/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1467 - accuracy: 0.9582 - val_loss: 0.1350 - val_accuracy: 0.9652\n",
            "Epoch 851/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1303 - accuracy: 0.9653 - val_loss: 0.1308 - val_accuracy: 0.9643\n",
            "Epoch 852/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1065 - accuracy: 0.9732 - val_loss: 0.1069 - val_accuracy: 0.9740\n",
            "Epoch 853/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0844 - accuracy: 0.9823 - val_loss: 0.0918 - val_accuracy: 0.9806\n",
            "Epoch 854/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0748 - accuracy: 0.9859 - val_loss: 0.0892 - val_accuracy: 0.9811\n",
            "Epoch 855/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0713 - accuracy: 0.9871 - val_loss: 0.0858 - val_accuracy: 0.9821\n",
            "Epoch 856/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0688 - accuracy: 0.9885 - val_loss: 0.0844 - val_accuracy: 0.9832\n",
            "Epoch 857/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0680 - accuracy: 0.9886 - val_loss: 0.0840 - val_accuracy: 0.9827\n",
            "Epoch 858/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0678 - accuracy: 0.9887 - val_loss: 0.0846 - val_accuracy: 0.9826\n",
            "Epoch 859/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0682 - accuracy: 0.9885 - val_loss: 0.0822 - val_accuracy: 0.9840\n",
            "Epoch 860/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0667 - accuracy: 0.9891 - val_loss: 0.0820 - val_accuracy: 0.9836\n",
            "Epoch 861/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0667 - accuracy: 0.9888 - val_loss: 0.0831 - val_accuracy: 0.9835\n",
            "Epoch 862/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0660 - accuracy: 0.9894 - val_loss: 0.0827 - val_accuracy: 0.9833\n",
            "Epoch 863/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0669 - accuracy: 0.9891 - val_loss: 0.0812 - val_accuracy: 0.9842\n",
            "Epoch 864/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0687 - accuracy: 0.9879 - val_loss: 0.0845 - val_accuracy: 0.9826\n",
            "Epoch 865/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0697 - accuracy: 0.9880 - val_loss: 0.0856 - val_accuracy: 0.9816\n",
            "Epoch 866/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0676 - accuracy: 0.9885 - val_loss: 0.0860 - val_accuracy: 0.9819\n",
            "Epoch 867/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0682 - accuracy: 0.9884 - val_loss: 0.0843 - val_accuracy: 0.9832\n",
            "Epoch 868/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0672 - accuracy: 0.9887 - val_loss: 0.0876 - val_accuracy: 0.9810\n",
            "Epoch 869/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0722 - accuracy: 0.9864 - val_loss: 0.0893 - val_accuracy: 0.9815\n",
            "Epoch 870/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0753 - accuracy: 0.9857 - val_loss: 0.0933 - val_accuracy: 0.9786\n",
            "Epoch 871/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0767 - accuracy: 0.9846 - val_loss: 0.0937 - val_accuracy: 0.9792\n",
            "Epoch 872/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0738 - accuracy: 0.9863 - val_loss: 0.0856 - val_accuracy: 0.9823\n",
            "Epoch 873/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0790 - accuracy: 0.9839 - val_loss: 0.0932 - val_accuracy: 0.9789\n",
            "Epoch 874/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0742 - accuracy: 0.9861 - val_loss: 0.0855 - val_accuracy: 0.9822\n",
            "Epoch 875/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0695 - accuracy: 0.9880 - val_loss: 0.0820 - val_accuracy: 0.9836\n",
            "Epoch 876/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0660 - accuracy: 0.9891 - val_loss: 0.0818 - val_accuracy: 0.9837\n",
            "Epoch 877/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0658 - accuracy: 0.9893 - val_loss: 0.0813 - val_accuracy: 0.9841\n",
            "Epoch 878/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0649 - accuracy: 0.9899 - val_loss: 0.0801 - val_accuracy: 0.9850\n",
            "Epoch 879/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0643 - accuracy: 0.9900 - val_loss: 0.0828 - val_accuracy: 0.9832\n",
            "Epoch 880/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0654 - accuracy: 0.9895 - val_loss: 0.0831 - val_accuracy: 0.9828\n",
            "Epoch 881/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0650 - accuracy: 0.9897 - val_loss: 0.0813 - val_accuracy: 0.9843\n",
            "Epoch 882/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0651 - accuracy: 0.9895 - val_loss: 0.0805 - val_accuracy: 0.9839\n",
            "Epoch 883/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0649 - accuracy: 0.9896 - val_loss: 0.0827 - val_accuracy: 0.9827\n",
            "Epoch 884/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0664 - accuracy: 0.9890 - val_loss: 0.0839 - val_accuracy: 0.9827\n",
            "Epoch 885/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0670 - accuracy: 0.9887 - val_loss: 0.0902 - val_accuracy: 0.9796\n",
            "Epoch 886/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0729 - accuracy: 0.9859 - val_loss: 0.0825 - val_accuracy: 0.9826\n",
            "Epoch 887/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0668 - accuracy: 0.9886 - val_loss: 0.0798 - val_accuracy: 0.9850\n",
            "Epoch 888/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0641 - accuracy: 0.9900 - val_loss: 0.0809 - val_accuracy: 0.9840\n",
            "Epoch 889/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0645 - accuracy: 0.9895 - val_loss: 0.0918 - val_accuracy: 0.9793\n",
            "Epoch 890/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0784 - accuracy: 0.9834 - val_loss: 0.1041 - val_accuracy: 0.9758\n",
            "Epoch 891/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1600 - accuracy: 0.9578 - val_loss: 0.1926 - val_accuracy: 0.9442\n",
            "Epoch 892/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.1598 - accuracy: 0.9550 - val_loss: 0.1259 - val_accuracy: 0.9678\n",
            "Epoch 893/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1063 - accuracy: 0.9736 - val_loss: 0.1094 - val_accuracy: 0.9729\n",
            "Epoch 894/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0905 - accuracy: 0.9793 - val_loss: 0.0936 - val_accuracy: 0.9788\n",
            "Epoch 895/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0776 - accuracy: 0.9846 - val_loss: 0.0861 - val_accuracy: 0.9823\n",
            "Epoch 896/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0709 - accuracy: 0.9875 - val_loss: 0.0859 - val_accuracy: 0.9821\n",
            "Epoch 897/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0698 - accuracy: 0.9876 - val_loss: 0.0819 - val_accuracy: 0.9833\n",
            "Epoch 898/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0671 - accuracy: 0.9889 - val_loss: 0.0838 - val_accuracy: 0.9822\n",
            "Epoch 899/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0670 - accuracy: 0.9890 - val_loss: 0.0798 - val_accuracy: 0.9844\n",
            "Epoch 900/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0647 - accuracy: 0.9895 - val_loss: 0.0802 - val_accuracy: 0.9841\n",
            "Epoch 901/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0659 - accuracy: 0.9891 - val_loss: 0.0815 - val_accuracy: 0.9836\n",
            "Epoch 902/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0662 - accuracy: 0.9890 - val_loss: 0.0807 - val_accuracy: 0.9844\n",
            "Epoch 903/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0674 - accuracy: 0.9885 - val_loss: 0.0805 - val_accuracy: 0.9841\n",
            "Epoch 904/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0654 - accuracy: 0.9894 - val_loss: 0.0804 - val_accuracy: 0.9838\n",
            "Epoch 905/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0652 - accuracy: 0.9893 - val_loss: 0.0845 - val_accuracy: 0.9830\n",
            "Epoch 906/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0653 - accuracy: 0.9893 - val_loss: 0.0786 - val_accuracy: 0.9848\n",
            "Epoch 907/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0632 - accuracy: 0.9900 - val_loss: 0.0787 - val_accuracy: 0.9847\n",
            "Epoch 908/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0634 - accuracy: 0.9902 - val_loss: 0.0815 - val_accuracy: 0.9837\n",
            "Epoch 909/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0662 - accuracy: 0.9888 - val_loss: 0.0909 - val_accuracy: 0.9796\n",
            "Epoch 910/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0793 - accuracy: 0.9833 - val_loss: 0.0945 - val_accuracy: 0.9778\n",
            "Epoch 911/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0754 - accuracy: 0.9854 - val_loss: 0.0823 - val_accuracy: 0.9832\n",
            "Epoch 912/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0671 - accuracy: 0.9885 - val_loss: 0.0795 - val_accuracy: 0.9847\n",
            "Epoch 913/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0648 - accuracy: 0.9896 - val_loss: 0.0810 - val_accuracy: 0.9835\n",
            "Epoch 914/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0626 - accuracy: 0.9904 - val_loss: 0.0774 - val_accuracy: 0.9850\n",
            "Epoch 915/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0652 - accuracy: 0.9890 - val_loss: 0.0785 - val_accuracy: 0.9855\n",
            "Epoch 916/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0633 - accuracy: 0.9903 - val_loss: 0.0776 - val_accuracy: 0.9852\n",
            "Epoch 917/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0636 - accuracy: 0.9899 - val_loss: 0.0806 - val_accuracy: 0.9841\n",
            "Epoch 918/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0638 - accuracy: 0.9897 - val_loss: 0.0995 - val_accuracy: 0.9772\n",
            "Epoch 919/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0792 - accuracy: 0.9831 - val_loss: 0.0886 - val_accuracy: 0.9811\n",
            "Epoch 920/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0817 - accuracy: 0.9829 - val_loss: 0.0935 - val_accuracy: 0.9798\n",
            "Epoch 921/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0820 - accuracy: 0.9829 - val_loss: 0.0982 - val_accuracy: 0.9769\n",
            "Epoch 922/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0757 - accuracy: 0.9851 - val_loss: 0.0846 - val_accuracy: 0.9822\n",
            "Epoch 923/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0695 - accuracy: 0.9879 - val_loss: 0.0811 - val_accuracy: 0.9835\n",
            "Epoch 924/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0653 - accuracy: 0.9894 - val_loss: 0.0811 - val_accuracy: 0.9834\n",
            "Epoch 925/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0636 - accuracy: 0.9900 - val_loss: 0.0790 - val_accuracy: 0.9848\n",
            "Epoch 926/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0626 - accuracy: 0.9903 - val_loss: 0.0770 - val_accuracy: 0.9853\n",
            "Epoch 927/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0621 - accuracy: 0.9903 - val_loss: 0.0774 - val_accuracy: 0.9854\n",
            "Epoch 928/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0621 - accuracy: 0.9904 - val_loss: 0.0840 - val_accuracy: 0.9822\n",
            "Epoch 929/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0640 - accuracy: 0.9898 - val_loss: 0.0777 - val_accuracy: 0.9850\n",
            "Epoch 930/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0622 - accuracy: 0.9905 - val_loss: 0.0773 - val_accuracy: 0.9852\n",
            "Epoch 931/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0647 - accuracy: 0.9893 - val_loss: 0.0827 - val_accuracy: 0.9834\n",
            "Epoch 932/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0714 - accuracy: 0.9865 - val_loss: 0.0954 - val_accuracy: 0.9773\n",
            "Epoch 933/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0746 - accuracy: 0.9853 - val_loss: 0.0880 - val_accuracy: 0.9812\n",
            "Epoch 934/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0718 - accuracy: 0.9867 - val_loss: 0.0922 - val_accuracy: 0.9789\n",
            "Epoch 935/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0703 - accuracy: 0.9869 - val_loss: 0.0778 - val_accuracy: 0.9849\n",
            "Epoch 936/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0668 - accuracy: 0.9884 - val_loss: 0.0854 - val_accuracy: 0.9820\n",
            "Epoch 937/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0694 - accuracy: 0.9872 - val_loss: 0.0895 - val_accuracy: 0.9804\n",
            "Epoch 938/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0698 - accuracy: 0.9871 - val_loss: 0.0844 - val_accuracy: 0.9823\n",
            "Epoch 939/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0675 - accuracy: 0.9880 - val_loss: 0.0778 - val_accuracy: 0.9849\n",
            "Epoch 940/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0616 - accuracy: 0.9906 - val_loss: 0.0781 - val_accuracy: 0.9850\n",
            "Epoch 941/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0632 - accuracy: 0.9900 - val_loss: 0.0788 - val_accuracy: 0.9843\n",
            "Epoch 942/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0628 - accuracy: 0.9900 - val_loss: 0.0791 - val_accuracy: 0.9849\n",
            "Epoch 943/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0630 - accuracy: 0.9898 - val_loss: 0.0803 - val_accuracy: 0.9835\n",
            "Epoch 944/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0668 - accuracy: 0.9882 - val_loss: 0.0844 - val_accuracy: 0.9819\n",
            "Epoch 945/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0650 - accuracy: 0.9888 - val_loss: 0.0774 - val_accuracy: 0.9848\n",
            "Epoch 946/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0634 - accuracy: 0.9896 - val_loss: 0.2046 - val_accuracy: 0.9435\n",
            "Epoch 947/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.2463 - accuracy: 0.9325 - val_loss: 0.1771 - val_accuracy: 0.9485\n",
            "Epoch 948/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.1380 - accuracy: 0.9624 - val_loss: 0.1079 - val_accuracy: 0.9735\n",
            "Epoch 949/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0936 - accuracy: 0.9784 - val_loss: 0.0982 - val_accuracy: 0.9772\n",
            "Epoch 950/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0755 - accuracy: 0.9852 - val_loss: 0.0832 - val_accuracy: 0.9834\n",
            "Epoch 951/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0658 - accuracy: 0.9890 - val_loss: 0.0806 - val_accuracy: 0.9833\n",
            "Epoch 952/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0642 - accuracy: 0.9897 - val_loss: 0.0774 - val_accuracy: 0.9851\n",
            "Epoch 953/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0626 - accuracy: 0.9903 - val_loss: 0.0824 - val_accuracy: 0.9830\n",
            "Epoch 954/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0648 - accuracy: 0.9894 - val_loss: 0.0768 - val_accuracy: 0.9856\n",
            "Epoch 955/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0615 - accuracy: 0.9908 - val_loss: 0.0785 - val_accuracy: 0.9844\n",
            "Epoch 956/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0617 - accuracy: 0.9902 - val_loss: 0.0768 - val_accuracy: 0.9850\n",
            "Epoch 957/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0608 - accuracy: 0.9908 - val_loss: 0.0829 - val_accuracy: 0.9827\n",
            "Epoch 958/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0657 - accuracy: 0.9885 - val_loss: 0.0773 - val_accuracy: 0.9847\n",
            "Epoch 959/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0609 - accuracy: 0.9907 - val_loss: 0.0761 - val_accuracy: 0.9855\n",
            "Epoch 960/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0606 - accuracy: 0.9910 - val_loss: 0.0770 - val_accuracy: 0.9853\n",
            "Epoch 961/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0617 - accuracy: 0.9904 - val_loss: 0.0765 - val_accuracy: 0.9852\n",
            "Epoch 962/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0612 - accuracy: 0.9906 - val_loss: 0.0753 - val_accuracy: 0.9859\n",
            "Epoch 963/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0627 - accuracy: 0.9901 - val_loss: 0.0822 - val_accuracy: 0.9827\n",
            "Epoch 964/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0628 - accuracy: 0.9902 - val_loss: 0.0845 - val_accuracy: 0.9815\n",
            "Epoch 965/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0660 - accuracy: 0.9882 - val_loss: 0.0807 - val_accuracy: 0.9834\n",
            "Epoch 966/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0706 - accuracy: 0.9862 - val_loss: 0.0857 - val_accuracy: 0.9815\n",
            "Epoch 967/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0664 - accuracy: 0.9883 - val_loss: 0.0821 - val_accuracy: 0.9841\n",
            "Epoch 968/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0651 - accuracy: 0.9889 - val_loss: 0.0794 - val_accuracy: 0.9837\n",
            "Epoch 969/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0631 - accuracy: 0.9897 - val_loss: 0.0766 - val_accuracy: 0.9852\n",
            "Epoch 970/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0614 - accuracy: 0.9906 - val_loss: 0.0756 - val_accuracy: 0.9860\n",
            "Epoch 971/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0603 - accuracy: 0.9910 - val_loss: 0.0753 - val_accuracy: 0.9855\n",
            "Epoch 972/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0597 - accuracy: 0.9909 - val_loss: 0.0755 - val_accuracy: 0.9858\n",
            "Epoch 973/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0604 - accuracy: 0.9908 - val_loss: 0.0770 - val_accuracy: 0.9854\n",
            "Epoch 974/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0582 - accuracy: 0.9917 - val_loss: 0.0752 - val_accuracy: 0.9855\n",
            "Epoch 975/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0592 - accuracy: 0.9913 - val_loss: 0.0757 - val_accuracy: 0.9854\n",
            "Epoch 976/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0584 - accuracy: 0.9915 - val_loss: 0.0782 - val_accuracy: 0.9844\n",
            "Epoch 977/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0613 - accuracy: 0.9903 - val_loss: 0.0736 - val_accuracy: 0.9863\n",
            "Epoch 978/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0582 - accuracy: 0.9916 - val_loss: 0.0809 - val_accuracy: 0.9829\n",
            "Epoch 979/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0641 - accuracy: 0.9889 - val_loss: 0.0750 - val_accuracy: 0.9857\n",
            "Epoch 980/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0610 - accuracy: 0.9907 - val_loss: 0.0749 - val_accuracy: 0.9857\n",
            "Epoch 981/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0595 - accuracy: 0.9909 - val_loss: 0.0775 - val_accuracy: 0.9847\n",
            "Epoch 982/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1104 - accuracy: 0.9753 - val_loss: 0.2475 - val_accuracy: 0.9323\n",
            "Epoch 983/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.2081 - accuracy: 0.9399 - val_loss: 0.1500 - val_accuracy: 0.9579\n",
            "Epoch 984/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.1271 - accuracy: 0.9661 - val_loss: 0.1144 - val_accuracy: 0.9719\n",
            "Epoch 985/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0915 - accuracy: 0.9792 - val_loss: 0.0908 - val_accuracy: 0.9801\n",
            "Epoch 986/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0739 - accuracy: 0.9861 - val_loss: 0.0796 - val_accuracy: 0.9843\n",
            "Epoch 987/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0654 - accuracy: 0.9895 - val_loss: 0.0789 - val_accuracy: 0.9838\n",
            "Epoch 988/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0621 - accuracy: 0.9902 - val_loss: 0.0773 - val_accuracy: 0.9848\n",
            "Epoch 989/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0613 - accuracy: 0.9906 - val_loss: 0.0757 - val_accuracy: 0.9853\n",
            "Epoch 990/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0604 - accuracy: 0.9907 - val_loss: 0.0734 - val_accuracy: 0.9863\n",
            "Epoch 991/1000\n",
            "11/11 [==============================] - 1s 79ms/step - loss: 0.0601 - accuracy: 0.9910 - val_loss: 0.0773 - val_accuracy: 0.9844\n",
            "Epoch 992/1000\n",
            "11/11 [==============================] - 1s 78ms/step - loss: 0.0613 - accuracy: 0.9902 - val_loss: 0.0780 - val_accuracy: 0.9844\n",
            "Epoch 993/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0628 - accuracy: 0.9898 - val_loss: 0.0746 - val_accuracy: 0.9860\n",
            "Epoch 994/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0608 - accuracy: 0.9912 - val_loss: 0.0739 - val_accuracy: 0.9861\n",
            "Epoch 995/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0596 - accuracy: 0.9911 - val_loss: 0.0739 - val_accuracy: 0.9860\n",
            "Epoch 996/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0578 - accuracy: 0.9914 - val_loss: 0.0741 - val_accuracy: 0.9861\n",
            "Epoch 997/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0581 - accuracy: 0.9914 - val_loss: 0.0727 - val_accuracy: 0.9865\n",
            "Epoch 998/1000\n",
            "11/11 [==============================] - 1s 76ms/step - loss: 0.0576 - accuracy: 0.9917 - val_loss: 0.0742 - val_accuracy: 0.9858\n",
            "Epoch 999/1000\n",
            "11/11 [==============================] - 1s 77ms/step - loss: 0.0597 - accuracy: 0.9911 - val_loss: 0.0723 - val_accuracy: 0.9872\n",
            "Epoch 1000/1000\n",
            "11/11 [==============================] - 1s 75ms/step - loss: 0.0576 - accuracy: 0.9920 - val_loss: 0.0758 - val_accuracy: 0.9848\n",
            "CPU times: user 14min 51s, sys: 43.9 s, total: 15min 35s\n",
            "Wall time: 14min 30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9YV8mgUXDwO"
      },
      "source": [
        "## 4) Visualization(Loss, Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSEBhQLJW0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "cc621bec-7047-4a26-b27b-423641e4a2a1"
      },
      "source": [
        "epochs = range(1, len(hist.history['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(epochs, hist.history['loss'])\n",
        "plt.plot(epochs, hist.history['val_loss'])\n",
        "\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "FvHLNFU0KfEu",
        "outputId": "ddb73dbb-c462-44a0-9326-3f665923b9eb"
      },
      "source": [
        "epochs = range(1, len(hist.history['loss']) + 1)\n",
        "\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(epochs, hist.history['accuracy'])\n",
        "plt.plot(epochs, hist.history['val_accuracy'])\n",
        "\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH43j2ZpXBbi"
      },
      "source": [
        "## 5) Model Evaluate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Bp9GHwXIG2",
        "outputId": "7669a961-9a81-4ee9-8c85-9d462db6e156"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test,\n",
        "                                batch_size = batch_size)\n",
        "\n",
        "print('Loss = {:.5f}'.format(loss))\n",
        "print('Accuracy = {:.5f}'.format(accuracy))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9856\n",
            "Loss = 0.07205\n",
            "Accuracy = 0.98565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjztcGV6YgcZ"
      },
      "source": [
        "## 6) Model Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg0630NOYt0e"
      },
      "source": [
        "abs_path = '/content/drive/MyDrive/Project/Project_Gotcha/models/'\n",
        "model_name = 'lstm_2_june_12_00.h5'\n",
        "final_path = abs_path + model_name"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYDr_nSBYhqJ"
      },
      "source": [
        "model.save(final_path)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vfo5idsZazR"
      },
      "source": [
        "# 3. Model Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71J-QkVsZEhz"
      },
      "source": [
        "## 1) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "FL_TV9eV9sFP",
        "outputId": "01ce1d6d-f2ee-4ab7-c387-45e7ade1d0e2"
      },
      "source": [
        "test_path = '/content/drive/MyDrive/Project/Project_Gotcha/ipynb/OpenPose/datasets/old/real_test.csv'\n",
        "\n",
        "test_sample = pd.read_csv(test_path)\n",
        "test_sample.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>131.090</td>\n",
              "      <td>158.404</td>\n",
              "      <td>104.906</td>\n",
              "      <td>183.220</td>\n",
              "      <td>110.116</td>\n",
              "      <td>183.185</td>\n",
              "      <td>162.316</td>\n",
              "      <td>193.695</td>\n",
              "      <td>204.122</td>\n",
              "      <td>201.516</td>\n",
              "      <td>97.0971</td>\n",
              "      <td>183.205</td>\n",
              "      <td>124.488</td>\n",
              "      <td>221.078</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>108.863</td>\n",
              "      <td>273.276</td>\n",
              "      <td>127.096</td>\n",
              "      <td>335.911</td>\n",
              "      <td>144.066</td>\n",
              "      <td>402.462</td>\n",
              "      <td>99.7320</td>\n",
              "      <td>270.668</td>\n",
              "      <td>110.185</td>\n",
              "      <td>329.394</td>\n",
              "      <td>97.0830</td>\n",
              "      <td>380.339</td>\n",
              "      <td>128.364</td>\n",
              "      <td>150.563</td>\n",
              "      <td>128.435</td>\n",
              "      <td>150.634</td>\n",
              "      <td>108.793</td>\n",
              "      <td>150.565</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>131.075</td>\n",
              "      <td>153.246</td>\n",
              "      <td>104.930</td>\n",
              "      <td>183.228</td>\n",
              "      <td>110.141</td>\n",
              "      <td>183.186</td>\n",
              "      <td>162.346</td>\n",
              "      <td>198.904</td>\n",
              "      <td>206.708</td>\n",
              "      <td>202.745</td>\n",
              "      <td>97.1013</td>\n",
              "      <td>183.205</td>\n",
              "      <td>117.939</td>\n",
              "      <td>223.647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.188</td>\n",
              "      <td>275.846</td>\n",
              "      <td>129.709</td>\n",
              "      <td>337.194</td>\n",
              "      <td>145.385</td>\n",
              "      <td>402.447</td>\n",
              "      <td>99.7283</td>\n",
              "      <td>273.283</td>\n",
              "      <td>110.202</td>\n",
              "      <td>329.406</td>\n",
              "      <td>97.0978</td>\n",
              "      <td>380.340</td>\n",
              "      <td>129.675</td>\n",
              "      <td>149.242</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>111.427</td>\n",
              "      <td>148.008</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>131.052</td>\n",
              "      <td>153.234</td>\n",
              "      <td>107.520</td>\n",
              "      <td>183.257</td>\n",
              "      <td>115.369</td>\n",
              "      <td>183.250</td>\n",
              "      <td>163.639</td>\n",
              "      <td>205.435</td>\n",
              "      <td>205.419</td>\n",
              "      <td>202.846</td>\n",
              "      <td>97.0939</td>\n",
              "      <td>183.219</td>\n",
              "      <td>125.801</td>\n",
              "      <td>223.715</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>114.041</td>\n",
              "      <td>274.578</td>\n",
              "      <td>132.325</td>\n",
              "      <td>335.921</td>\n",
              "      <td>147.994</td>\n",
              "      <td>402.449</td>\n",
              "      <td>99.6768</td>\n",
              "      <td>271.959</td>\n",
              "      <td>111.434</td>\n",
              "      <td>329.372</td>\n",
              "      <td>98.3188</td>\n",
              "      <td>380.352</td>\n",
              "      <td>129.680</td>\n",
              "      <td>149.272</td>\n",
              "      <td>131.060</td>\n",
              "      <td>151.908</td>\n",
              "      <td>111.436</td>\n",
              "      <td>149.258</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>137.531</td>\n",
              "      <td>153.186</td>\n",
              "      <td>110.139</td>\n",
              "      <td>183.290</td>\n",
              "      <td>120.575</td>\n",
              "      <td>184.531</td>\n",
              "      <td>158.432</td>\n",
              "      <td>222.338</td>\n",
              "      <td>198.883</td>\n",
              "      <td>213.235</td>\n",
              "      <td>99.6877</td>\n",
              "      <td>183.223</td>\n",
              "      <td>127.118</td>\n",
              "      <td>223.729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>119.318</td>\n",
              "      <td>274.603</td>\n",
              "      <td>140.127</td>\n",
              "      <td>337.208</td>\n",
              "      <td>150.532</td>\n",
              "      <td>402.451</td>\n",
              "      <td>103.6160</td>\n",
              "      <td>271.968</td>\n",
              "      <td>114.061</td>\n",
              "      <td>328.141</td>\n",
              "      <td>97.1001</td>\n",
              "      <td>381.564</td>\n",
              "      <td>131.072</td>\n",
              "      <td>147.994</td>\n",
              "      <td>140.091</td>\n",
              "      <td>150.609</td>\n",
              "      <td>116.664</td>\n",
              "      <td>149.275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>140.158</td>\n",
              "      <td>153.219</td>\n",
              "      <td>111.470</td>\n",
              "      <td>183.291</td>\n",
              "      <td>121.878</td>\n",
              "      <td>184.541</td>\n",
              "      <td>149.284</td>\n",
              "      <td>224.998</td>\n",
              "      <td>192.374</td>\n",
              "      <td>223.621</td>\n",
              "      <td>101.0190</td>\n",
              "      <td>183.244</td>\n",
              "      <td>128.389</td>\n",
              "      <td>230.197</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.917</td>\n",
              "      <td>277.190</td>\n",
              "      <td>141.439</td>\n",
              "      <td>337.226</td>\n",
              "      <td>150.589</td>\n",
              "      <td>401.232</td>\n",
              "      <td>107.5290</td>\n",
              "      <td>275.861</td>\n",
              "      <td>117.926</td>\n",
              "      <td>329.426</td>\n",
              "      <td>98.3216</td>\n",
              "      <td>381.571</td>\n",
              "      <td>137.537</td>\n",
              "      <td>148.007</td>\n",
              "      <td>140.221</td>\n",
              "      <td>150.621</td>\n",
              "      <td>120.601</td>\n",
              "      <td>146.688</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>assault-punch_flip</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3  ...       33   34   35               index\n",
              "0  131.090  158.404  104.906  183.220  ...  150.565  0.0  0.0  assault-punch_flip\n",
              "1  131.075  153.246  104.930  183.228  ...  148.008  0.0  0.0  assault-punch_flip\n",
              "2  131.052  153.234  107.520  183.257  ...  149.258  0.0  0.0  assault-punch_flip\n",
              "3  137.531  153.186  110.139  183.290  ...  149.275  0.0  0.0  assault-punch_flip\n",
              "4  140.158  153.219  111.470  183.291  ...  146.688  0.0  0.0  assault-punch_flip\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn-hwzUqWLvM",
        "outputId": "af93a94f-51aa-478a-f5ec-04e799287ee9"
      },
      "source": [
        "test_sample = deleteJoint(test_sample)\n",
        "test_sample = makeJointXY(test_sample)\n",
        "test_sample = jointAngles(test_sample)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32it [00:00, 66345.89it/s]\n",
            "32it [00:00, 50840.05it/s]\n",
            "32it [00:00, 49362.90it/s]\n",
            "32it [00:00, 57089.63it/s]\n",
            "32it [00:00, 93924.23it/s]\n",
            "32it [00:00, 19168.48it/s]\n",
            "32it [00:00, 28026.25it/s]\n",
            "32it [00:00, 65027.97it/s]\n",
            "32it [00:00, 53751.59it/s]\n",
            "32it [00:00, 53816.25it/s]\n",
            "32it [00:00, 97541.95it/s]\n",
            "32it [00:00, 11299.69it/s]\n",
            "32it [00:00, 18115.50it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 91992.96it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 96768.37it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 91867.03it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 54032.90it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 47376.54it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 60214.32it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 87609.48it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 62924.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "niF4-G5yWyDq",
        "outputId": "9312d5ca-0411-4547-a323-259f80aceeab"
      },
      "source": [
        "test_sample.head()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Ang_RShoulder</th>\n",
              "      <th>Ang_RElbow</th>\n",
              "      <th>Ang_RHip</th>\n",
              "      <th>Ang_RKnee</th>\n",
              "      <th>Ang_LShoulder</th>\n",
              "      <th>Ang_LElbow</th>\n",
              "      <th>Ang_LHip</th>\n",
              "      <th>Ang_LKnee</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.936190</td>\n",
              "      <td>-3.127849</td>\n",
              "      <td>-2.902233</td>\n",
              "      <td>-3.175192</td>\n",
              "      <td>-0.942728</td>\n",
              "      <td>-0.113306</td>\n",
              "      <td>-2.906344</td>\n",
              "      <td>-3.569468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.841084</td>\n",
              "      <td>-2.935511</td>\n",
              "      <td>-2.890232</td>\n",
              "      <td>-3.213897</td>\n",
              "      <td>-1.092086</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>-2.899397</td>\n",
              "      <td>-3.577908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>2.709890</td>\n",
              "      <td>-2.648894</td>\n",
              "      <td>-2.923201</td>\n",
              "      <td>-3.199961</td>\n",
              "      <td>-0.950486</td>\n",
              "      <td>-0.104408</td>\n",
              "      <td>-2.851410</td>\n",
              "      <td>-3.595384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>-3.807971</td>\n",
              "      <td>-2.135505</td>\n",
              "      <td>-2.920881</td>\n",
              "      <td>-3.304341</td>\n",
              "      <td>-0.969132</td>\n",
              "      <td>-0.078561</td>\n",
              "      <td>-2.884323</td>\n",
              "      <td>-3.632855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>assault-punch_flip</td>\n",
              "      <td>-3.997458</td>\n",
              "      <td>-2.134255</td>\n",
              "      <td>-2.938008</td>\n",
              "      <td>-3.313986</td>\n",
              "      <td>-1.038531</td>\n",
              "      <td>-0.019006</td>\n",
              "      <td>-2.907328</td>\n",
              "      <td>-3.692921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                index  Ang_RShoulder  ...  Ang_LHip  Ang_LKnee\n",
              "0  assault-punch_flip       2.936190  ... -2.906344  -3.569468\n",
              "1  assault-punch_flip       2.841084  ... -2.899397  -3.577908\n",
              "2  assault-punch_flip       2.709890  ... -2.851410  -3.595384\n",
              "3  assault-punch_flip      -3.807971  ... -2.884323  -3.632855\n",
              "4  assault-punch_flip      -3.997458  ... -2.907328  -3.692921\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W9uMsB9XhZR",
        "outputId": "e667d7e7-7b6d-4fb7-e8ac-1e04100eb77c"
      },
      "source": [
        "X_pre = makeX(test_sample)\n",
        "\n",
        "X_pre.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 21816.93it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 32, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZ8Zbo7ZH4d"
      },
      "source": [
        "## 2) Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMp0HbxoZKth"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "np.set_printoptions(precision=6, suppress=True) # np.array 표현 방식 변경 (지수 -> 실수)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whf3h3DVZSyU"
      },
      "source": [
        "loaded_model = load_model(final_path)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDxW_s0TK6jP",
        "outputId": "6b30315f-1551-4c71-f903-2e43e3aed4b0"
      },
      "source": [
        "predict_dict = {}\n",
        "\n",
        "predictions = loaded_model.predict(X_pre)\n",
        "\n",
        "for idx, c in enumerate(classes):\n",
        "    predict_dict[c] = predictions[0][idx]\n",
        "\n",
        "print(predict_dict)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.0054610227, 1: 0.0020778452, 2: 0.9817147, 3: 0.0027423487, 4: 0.004755246, 5: 0.0032487977}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_bCNdlPMRJO",
        "outputId": "908cc4ed-f06b-4e90-a9f6-478670d8ebb8"
      },
      "source": [
        "predict_class = np.argmax(predictions[0], axis=-1)\n",
        "\n",
        "classes[predict_class]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHo8krbynhsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}